{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP ON TRANSIT TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\",999)\n",
    "pd.set_option(\"display.max_rows\",999)\n",
    "pd.set_option(\"display.max_columns\",999)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017 = pd.read_json('df_sf_2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017.transit.fillna(value='None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017[\"transit_new\"] = df_sf_2017['transit'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LemmaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data - running NLP on transit column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(start_month, end_month):\n",
    "    df_X_train = df_sf_2017[(df_sf_2017['month'] >= start_month) & (df_sf_2017['month'] < end_month)]['transit_new']\n",
    "    y_train = df_sf_2017[(df_sf_2017['month'] >= start_month) & (df_sf_2017['month'] < end_month)]['popular']\n",
    "\n",
    "    df_X_test = df_sf_2017[df_sf_2017['month'] == end_month]['transit_new']\n",
    "    y_test = df_sf_2017[df_sf_2017['month'] == end_month]['popular']\n",
    "    \n",
    "    return df_X_train, y_train, df_X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.91 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_tf_vec(df_X_train, df_X_test):\n",
    "    tf_vectorizer_train = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english').fit(df_X_train)\n",
    "    X_train = tf_vectorizer_train.transform(df_X_train)\n",
    "    tf_vectorizer_test = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', vocabulary = tf_vectorizer_train.vocabulary_).fit(df_X_test)\n",
    "    X_test = tf_vectorizer_test.transform(df_X_test)\n",
    "    return X_train, X_test, tf_vectorizer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_nb(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    pickle.dump(nb, open('nb_model'+ str(model_num) + '.p', 'wb'))\n",
    "    preds = nb.predict(X_test)\n",
    "    scores_tf_nb[0][model_num] = accuracy_score(y_test, preds)\n",
    "    scores_tf_nb[1][model_num] = recall_score(y_test, nb.predict(X_test))\n",
    "    scores_tf_nb[2][model_num] = precision_score(y_test, nb.predict(X_test))\n",
    "    scores_tf_nb[3][model_num] = f1_score(y_test, nb.predict(X_test))\n",
    "    return scores_tf_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_searching (param_grid, model):\n",
    "\n",
    "    grid_search = GridSearchCV(model, \n",
    "                               param_grid=param_grid, cv=5, \n",
    "                               n_jobs=-1, scoring=make_scorer(f1_score))\n",
    "    fit = grid_search.fit(X_train, y_train)\n",
    "    predicted = fit.predict(X_test)\n",
    "    return grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the CountVectorizer, run with RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_rf(X_train, y_train, X_test, y_test):\n",
    "#     param_grid = {'n_estimators': [500, 1000, 1500], \n",
    "#                   'max_features': ['auto'], \n",
    "#                   'max_depth': [None, 10, 5],\n",
    "#                   'class_weight': [None, 'balanced']}\n",
    "\n",
    "#     model = RandomForestClassifier()\n",
    "    \n",
    "#     best_parameters = grid_searching(param_grid, model)\n",
    "#     rf = RandomForestClassifier(n_estimators = best_parameters['n_estimators'], \n",
    "#                                 n_jobs = -1, \n",
    "#                                 random_state = 0, \n",
    "#                                 max_features = ['auto'], \n",
    "#                                 max_depth = best_parameters['max_depth'], \n",
    "#                                 class_weight = best_parameters['class_weight'])\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators = 500, \n",
    "                                n_jobs = -1, \n",
    "                                random_state = 0, \n",
    "                                max_depth = 10)\n",
    "    rf.fit(X_train, y_train)\n",
    "    predicted = rf.predict(X_test)\n",
    "#     pickle.dump(rf, open('rf_nlp_countvec_50' + str(model_num) + '.p', 'wb'))\n",
    "    scores_tf_rf[0][model_num] = accuracy_score(y_test, predicted)\n",
    "    scores_tf_rf[1][model_num] = recall_score(y_test, predicted)\n",
    "    scores_tf_rf[2][model_num] = precision_score(y_test, predicted)\n",
    "    scores_tf_rf[3][model_num] = f1_score(y_test, predicted)\n",
    "    return scores_tf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tf_nb = np.zeros(shape=(4,9))\n",
    "scores_tf_rf = np.zeros(shape=(4,9))\n",
    "\n",
    "model_num = 0\n",
    "start_month = 1\n",
    "end_month = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note: interrupted due to low scores in the first couple of test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 0\n",
      "tf_nb\n",
      "[[0.85786627 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.34571788 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.73691275 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.47063866 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.82230406 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.0302267  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.92307692 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.05853659 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "2 5 1\n",
      "tf_nb\n",
      "[[0.85786627 0.86100917 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.34571788 0.34782609 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.73691275 0.75720165 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.47063866 0.47668394 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.82230406 0.82224771 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.0302267  0.02331443 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.92307692 1.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.05853659 0.0455665  0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "3 6 2\n",
      "tf_nb\n",
      "[[0.85786627 0.86100917 0.85128672 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.34571788 0.34782609 0.34356197 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.73691275 0.75720165 0.7264631  0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.47063866 0.47668394 0.46650327 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.82230406 0.82224771 0.81632885 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.0302267  0.02331443 0.03249097 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.92307692 1.         0.91525424 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.05853659 0.0455665  0.06275421 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mrun_tf_vec\u001b[0;34m(df_X_train, df_X_test)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-1031af763d5d>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, articles)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwnl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwnl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     return [token for sent in sentences\n\u001b[0m\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[0;32m--> 130\u001b[0;31m             for token in _treebank_word_tokenizer.tokenize(sent)]\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tokenize/treebank.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENDING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCONTRACTIONS2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while end_month <13:\n",
    "    df_X_train, y_train, df_X_test, y_test = split_data(start_month, end_month)\n",
    "    X_train, X_test, tf_vectorizer_train = run_tf_vec(df_X_train, df_X_test)\n",
    "    scores_tf_nb = predict_tf_nb(X_train, y_train, X_test, y_test)\n",
    "    scores_tf_rf = predict_tf_rf(X_train, y_train, X_test, y_test)\n",
    "    print(start_month, end_month, model_num)\n",
    "    model_num += 1\n",
    "    start_month += 1\n",
    "    end_month += 1\n",
    "    print('tf_nb')\n",
    "    print(scores_tf_nb)\n",
    "    print('tf_rf')\n",
    "    print(scores_tf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COUNT VECTORIZER NAIVE BAYES SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2855735738720989\n",
      "0.11523399384593074\n",
      "0.24673083356600964\n",
      "0.15709176316264284\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_nb = np.mean(scores_tf_nb[0])\n",
    "recall_2017_tf_nb = np.mean(scores_tf_nb[1])\n",
    "precision_2017_tf_nb = np.mean(scores_tf_nb[2])\n",
    "f1_score_2017_tf_nb = np.mean(scores_tf_nb[3])\n",
    "print(accuracy_2017_tf_nb)\n",
    "print(recall_2017_tf_nb)\n",
    "print(precision_2017_tf_nb)\n",
    "print(f1_score_2017_tf_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COUNT VECTORIZER RANDOM FOREST SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2734311803894418\n",
      "0.009559122746975773\n",
      "0.315370128929451\n",
      "0.018539700055106873\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_rf = np.mean(scores_tf_rf[0])\n",
    "recall_2017_tf_rf = np.mean(scores_tf_rf[1])\n",
    "precision_2017_tf_rf = np.mean(scores_tf_rf[2])\n",
    "f1_score_2017_tf_rf = np.mean(scores_tf_rf[3])\n",
    "print(accuracy_2017_tf_rf)\n",
    "print(recall_2017_tf_rf)\n",
    "print(precision_2017_tf_rf)\n",
    "print(f1_score_2017_tf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try running with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tf_idf_vec(df_X_train, df_X_test):\n",
    "    tf_idf_vectorizer_train = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words='english')\n",
    "    X_train2 = tf_idf_vectorizer_train.fit_transform(df_X_train).toarray()\n",
    "    tf_idf_vectorizer_test = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', vocabulary = tf_idf_vectorizer_train.vocabulary_)\n",
    "    X_test2 = tf_idf_vectorizer_test.fit_transform(df_X_test).toarray()\n",
    "    return X_train2, X_test2, tf_idf_vectorizer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf_nb(X_train2, y_train, X_test2, y_test):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train2, y_train)\n",
    "    # pickle.dump(nb, open('nb_model_guassian.p', 'wb'))\n",
    "    preds = nb.predict(X_test2)\n",
    "    scores_tf_idf_nb[0][model_num] = accuracy_score(y_test, preds)\n",
    "    scores_tf_idf_nb[1][model_num] = recall_score(y_test, preds)\n",
    "    scores_tf_idf_nb[2][model_num] = precision_score(y_test, preds)\n",
    "    scores_tf_idf_nb[3][model_num] = f1_score(y_test, preds)\n",
    "    return scores_tf_idf_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the TF-IDF, run with RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf_rf(X_train2, y_train, X_test2, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators = 10, n_jobs=-1, random_state=0, class_weight = {0:.95, 1:.05})\n",
    "    rf.fit(X_train2, y_train)\n",
    "    predicted = rf.predict(X_test2)\n",
    "#     pickle.dump(rf, open('rf_nlp_50.p', 'wb'))\n",
    "    scores_tf_idf_rf[0][model_num] = accuracy_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[1][model_num] = recall_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[2][model_num] = precision_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[3][model_num] = f1_score(y_test, predicted)\n",
    "    return scores_tf_idf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tf_idf_nb = np.zeros(shape=(4,9))\n",
    "scores_tf_idf_rf = np.zeros(shape=(4,9))\n",
    "\n",
    "model_num = 0\n",
    "start_month = 1\n",
    "end_month = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 0\n",
      "tf_idf_nb\n",
      "[[0.35815399 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.96851385 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.21769285 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.35548365 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.91989872 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.59949622 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.94071146 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.73230769 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "2 5 1\n",
      "tf_idf_nb\n",
      "[[0.35815399 0.35837156 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.96851385 0.97353497 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.21769285 0.21766695 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.35548365 0.35578584 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.91989872 0.91731651 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.59949622 0.58727158 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.94071146 0.93386774 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.73230769 0.72108317 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "3 6 2\n",
      "tf_idf_nb\n",
      "[[0.35815399 0.35837156 0.36335687 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.96851385 0.97353497 0.9765343  0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.21769285 0.21766695 0.22620209 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.35548365 0.35578584 0.36731923 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.91989872 0.91731651 0.91277613 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.59949622 0.58727158 0.58423586 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.94071146 0.93386774 0.92829828 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.73230769 0.72108317 0.71713442 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "4 7 3\n",
      "tf_idf_nb\n",
      "[[0.35815399 0.35837156 0.36335687 0.3710827  0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.96851385 0.97353497 0.9765343  0.9749709  0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.21769285 0.21766695 0.22620209 0.23293005 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.35548365 0.35578584 0.36731923 0.37602425 0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.91989872 0.91731651 0.91277613 0.90768186 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.59949622 0.58727158 0.58423586 0.56984866 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.94071146 0.93386774 0.92829828 0.92708333 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.73230769 0.72108317 0.71713442 0.70583994 0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "5 8 4\n",
      "tf_idf_nb\n",
      "[[0.35815399 0.35837156 0.36335687 0.3710827  0.36667404 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.96851385 0.97353497 0.9765343  0.9749709  0.96039067 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.21769285 0.21766695 0.22620209 0.23293005 0.23851233 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.35548365 0.35578584 0.36731923 0.37602425 0.38212435 0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.91989872 0.91731651 0.91277613 0.90768186 0.90241204 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.59949622 0.58727158 0.58423586 0.56984866 0.55290288 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.94071146 0.93386774 0.92829828 0.92708333 0.9461467  0.\n",
      "  0.         0.         0.        ]\n",
      " [0.73230769 0.72108317 0.71713442 0.70583994 0.69794521 0.\n",
      "  0.         0.         0.        ]]\n",
      "6 9 5\n",
      "tf_idf_nb\n",
      "[[0.35815399 0.35837156 0.36335687 0.3710827  0.36667404 0.39137587\n",
      "  0.         0.         0.        ]\n",
      " [0.96851385 0.97353497 0.9765343  0.9749709  0.96039067 0.97333333\n",
      "  0.         0.         0.        ]\n",
      " [0.21769285 0.21766695 0.22620209 0.23293005 0.23851233 0.26240115\n",
      "  0.         0.         0.        ]\n",
      " [0.35548365 0.35578584 0.36731923 0.37602425 0.38212435 0.41336353\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.91989872 0.91731651 0.91277613 0.90768186 0.90241204 0.89730936\n",
      "  0.         0.         0.        ]\n",
      " [0.59949622 0.58727158 0.58423586 0.56984866 0.55290288 0.54826667\n",
      "  0.         0.         0.        ]\n",
      " [0.94071146 0.93386774 0.92829828 0.92708333 0.9461467  0.97440758\n",
      "  0.         0.         0.        ]\n",
      " [0.73230769 0.72108317 0.71713442 0.70583994 0.69794521 0.70170648\n",
      "  0.         0.         0.        ]]\n",
      "7 10 6\n",
      "tf_idf_nb\n",
      "[[0.35815399 0.35837156 0.36335687 0.3710827  0.36667404 0.39137587\n",
      "  0.38081853 0.         0.        ]\n",
      " [0.96851385 0.97353497 0.9765343  0.9749709  0.96039067 0.97333333\n",
      "  0.97411516 0.         0.        ]\n",
      " [0.21769285 0.21766695 0.22620209 0.23293005 0.23851233 0.26240115\n",
      "  0.25253355 0.         0.        ]\n",
      " [0.35548365 0.35578584 0.36731923 0.37602425 0.38212435 0.41336353\n",
      "  0.40108755 0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.91989872 0.91731651 0.91277613 0.90768186 0.90241204 0.89730936\n",
      "  0.89970767 0.         0.        ]\n",
      " [0.59949622 0.58727158 0.58423586 0.56984866 0.55290288 0.54826667\n",
      "  0.56840993 0.         0.        ]\n",
      " [0.94071146 0.93386774 0.92829828 0.92708333 0.9461467  0.97440758\n",
      "  0.93483927 0.         0.        ]\n",
      " [0.73230769 0.72108317 0.71713442 0.70583994 0.69794521 0.70170648\n",
      "  0.70696452 0.         0.        ]]\n",
      "8 11 7\n",
      "tf_idf_nb\n",
      "[[0.35815399 0.35837156 0.36335687 0.3710827  0.36667404 0.39137587\n",
      "  0.38081853 0.41034614 0.        ]\n",
      " [0.96851385 0.97353497 0.9765343  0.9749709  0.96039067 0.97333333\n",
      "  0.97411516 0.97282174 0.        ]\n",
      " [0.21769285 0.21766695 0.22620209 0.23293005 0.23851233 0.26240115\n",
      "  0.25253355 0.28487828 0.        ]\n",
      " [0.35548365 0.35578584 0.36731923 0.37602425 0.38212435 0.41336353\n",
      "  0.40108755 0.44070252 0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.91989872 0.91731651 0.91277613 0.90768186 0.90241204 0.89730936\n",
      "  0.89970767 0.89742937 0.        ]\n",
      " [0.59949622 0.58727158 0.58423586 0.56984866 0.55290288 0.54826667\n",
      "  0.56840993 0.6195044  0.        ]\n",
      " [0.94071146 0.93386774 0.92829828 0.92708333 0.9461467  0.97440758\n",
      "  0.93483927 0.92666401 0.        ]\n",
      " [0.73230769 0.72108317 0.71713442 0.70583994 0.69794521 0.70170648\n",
      "  0.70696452 0.74257426 0.        ]]\n",
      "9 12 8\n",
      "tf_idf_nb\n",
      "[[0.35815399 0.35837156 0.36335687 0.3710827  0.36667404 0.39137587\n",
      "  0.38081853 0.41034614 0.4390462 ]\n",
      " [0.96851385 0.97353497 0.9765343  0.9749709  0.96039067 0.97333333\n",
      "  0.97411516 0.97282174 0.97558269]\n",
      " [0.21769285 0.21766695 0.22620209 0.23293005 0.23851233 0.26240115\n",
      "  0.25253355 0.28487828 0.32092004]\n",
      " [0.35548365 0.35578584 0.36731923 0.37602425 0.38212435 0.41336353\n",
      "  0.40108755 0.44070252 0.48296703]]\n",
      "tf_idf_rf\n",
      "[[0.91989872 0.91731651 0.91277613 0.90768186 0.90241204 0.89730936\n",
      "  0.89970767 0.89742937 0.89605067]\n",
      " [0.59949622 0.58727158 0.58423586 0.56984866 0.55290288 0.54826667\n",
      "  0.56840993 0.6195044  0.64927858]\n",
      " [0.94071146 0.93386774 0.92829828 0.92708333 0.9461467  0.97440758\n",
      "  0.93483927 0.92666401 0.94698503]\n",
      " [0.73230769 0.72108317 0.71713442 0.70583994 0.69794521 0.70170648\n",
      "  0.70696452 0.74257426 0.77037037]]\n",
      "CPU times: user 30min 25s, sys: 16.2 s, total: 30min 41s\n",
      "Wall time: 8min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while end_month <13:\n",
    "    df_X_train, y_train, df_X_test, y_test = split_data(start_month, end_month) \n",
    "    X_train2, X_test2, tf_idf_vectorizer_train = run_tf_idf_vec(df_X_train, df_X_test)\n",
    "    scores_tf_idf_nb = predict_tf_idf_nb(X_train2, y_train, X_test2, y_test)\n",
    "    scores_tf_idf_rf = predict_tf_idf_rf(X_train2, y_train, X_test2, y_test)\n",
    "    print(start_month, end_month, model_num)\n",
    "    model_num += 1\n",
    "    start_month += 1\n",
    "    end_month += 1\n",
    "    print('tf_idf_nb')\n",
    "    print(scores_tf_idf_nb)\n",
    "    print('tf_idf_rf')\n",
    "    print(scores_tf_idf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF NAIVE BAYES SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38213621088603855\n",
      "0.9721997342580853\n",
      "0.25041525512359164\n",
      "0.39720643767821445\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[0])\n",
    "recall_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[1])\n",
    "precision_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[2])\n",
    "f1_score_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[3])\n",
    "print(accuracy_2017_tf_idf_nb)\n",
    "print(recall_2017_tf_idf_nb)\n",
    "print(precision_2017_tf_idf_nb)\n",
    "print(f1_score_2017_tf_idf_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF RANDOM FOREST SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.905620260692705\n",
      "0.5865794193870963\n",
      "0.939889267025484\n",
      "0.7217695624005519\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[0])\n",
    "recall_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[1])\n",
    "precision_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[2])\n",
    "f1_score_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[3])\n",
    "print(accuracy_2017_tf_idf_rf)\n",
    "print(recall_2017_tf_idf_rf)\n",
    "print(precision_2017_tf_idf_rf)\n",
    "print(f1_score_2017_tf_idf_rf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
