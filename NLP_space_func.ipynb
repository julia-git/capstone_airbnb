{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline score -  randomly guessing given that we know that 20% of the data is popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer, recall_score, precision_score, f1_score\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\",999)\n",
    "pd.set_option(\"display.max_rows\",999)\n",
    "pd.set_option(\"display.max_columns\",999)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_guessing(n_simulations = 10):\n",
    "    \n",
    "    count = 0 #\n",
    "    #keep track of \n",
    "    accuracy_baseline = np.zeros(shape=(1,9*n_simulations)) #we have 9 test sets for 2017. Multiply by 9 for each simulation.  \n",
    "    recall_baseline = np.zeros(shape=(1,9*n_simulations))\n",
    "    precision_baseline = np.zeros(shape=(1,9*n_simulations))\n",
    "    f1_baseline = np.zeros(shape=(1,9*n_simulations))\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        my_list = [True for x in range(2)] +  [False for x in range(8)]\n",
    "        start_month = 1\n",
    "        end_month = 4\n",
    "        while end_month <13:\n",
    "            y_test = df_sf_2017[df_sf_2017['month'] == end_month]['popular']\n",
    "            y_pred = pd.Series(random.choice(my_list) for x in range(y_test.size))\n",
    "            accuracy_baseline[0][count] = accuracy_score(y_test, y_pred)\n",
    "            recall_baseline[0][count] = recall_score(y_test, y_pred)\n",
    "            precision_baseline[0][count] = precision_score(y_test, y_pred)\n",
    "            f1_baseline[0][count] = f1_score(y_test, y_pred)\n",
    "            count+=1\n",
    "            start_month += 1\n",
    "            end_month += 1\n",
    "            \n",
    "    return accuracy_baseline, recall_baseline, precision_baseline, f1_baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6738568003192421\n",
      "0.19992382244534446\n",
      "0.2098987145097955\n",
      "0.20396154681655437\n"
     ]
    }
   ],
   "source": [
    "accuracy_baseline, recall_baseline, precision_baseline, f1_baseline = randomly_guessing(n_simulations = 1000)\n",
    "print(accuracy_baseline.mean())\n",
    "print(recall_baseline.mean()) \n",
    "print(precision_baseline.mean()) \n",
    "print(f1_baseline.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sf_2017 = pickle.load(open('../data_sf_2017.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017 = pd.read_json('df_sf_2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117262"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sf_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017.space.fillna(value='None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017[\"space_new\"] = df_sf_2017['space'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'floor': 30,\n",
       " 'breath': 15,\n",
       " 'taking': 86,\n",
       " 'view': 96,\n",
       " 'easy': 25,\n",
       " 'transportation': 91,\n",
       " 'downtown': 24,\n",
       " 'great': 35,\n",
       " 'neighborhood': 55,\n",
       " 'san': 76,\n",
       " 'francisco': 31,\n",
       " 'safe': 75,\n",
       " 'free': 32,\n",
       " 'parking': 59,\n",
       " 'located': 46,\n",
       " 'geographically': 34,\n",
       " 'center': 17,\n",
       " 'city': 18,\n",
       " 'public': 68,\n",
       " 'available': 10,\n",
       " '15': 1,\n",
       " 'minute': 51,\n",
       " 'private': 67,\n",
       " 'bathroom': 11,\n",
       " 'room': 74,\n",
       " 'street': 83,\n",
       " 'lovely': 48,\n",
       " 'noe': 57,\n",
       " 'valley': 94,\n",
       " 'access': 5,\n",
       " 'muni': 54,\n",
       " 'market': 49,\n",
       " 'perfectly': 63,\n",
       " 'enjoy': 26,\n",
       " 'peaceful': 61,\n",
       " 'environment': 28,\n",
       " 'want': 97,\n",
       " 'close': 20,\n",
       " 'shopping': 81,\n",
       " 'restaurant': 72,\n",
       " 'nightlife': 56,\n",
       " 'block': 14,\n",
       " 'j': 41,\n",
       " 'train': 90,\n",
       " '20m': 3,\n",
       " 'ride': 73,\n",
       " '3': 4,\n",
       " 'mission': 52,\n",
       " 'apartment': 8,\n",
       " '2': 2,\n",
       " 'bedroom': 12,\n",
       " '1': 0,\n",
       " 'recently': 70,\n",
       " 'remodeled': 71,\n",
       " 'modern': 53,\n",
       " 'kitchen': 42,\n",
       " 'dishwasher': 23,\n",
       " 'washerdryer': 98,\n",
       " 'building': 16,\n",
       " 'use': 93,\n",
       " 'best': 13,\n",
       " 'hill': 38,\n",
       " 'climb': 19,\n",
       " 'sunny': 85,\n",
       " 'airy': 6,\n",
       " 'minimalist': 50,\n",
       " 'terrarium': 87,\n",
       " 'various': 95,\n",
       " 'plant': 66,\n",
       " 'scattered': 78,\n",
       " 'entire': 27,\n",
       " 'place': 65,\n",
       " 'including': 40,\n",
       " 'living': 45,\n",
       " 'dining': 22,\n",
       " 'garage': 33,\n",
       " 'love': 47,\n",
       " 'spend': 82,\n",
       " 'time': 88,\n",
       " 'guest': 36,\n",
       " 'listed': 44,\n",
       " 'im': 39,\n",
       " 'town': 89,\n",
       " 'reachable': 69,\n",
       " 'phone': 64,\n",
       " 'anytime': 7,\n",
       " 'offer': 58,\n",
       " 'suggestionsadvice': 84,\n",
       " 'arrive': 9,\n",
       " 'favorite': 29,\n",
       " 'share': 80,\n",
       " 'truly': 92,\n",
       " 'sense': 79,\n",
       " 'community': 21,\n",
       " 'people': 62,\n",
       " 'like': 43,\n",
       " 'say': 77,\n",
       " 'hello': 37,\n",
       " 'passing': 60}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf_vectorizer_train.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LemmaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data - running NLP on \"access\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(start_month, end_month):\n",
    "    df_X_train = df_sf_2017[(df_sf_2017['month'] >= start_month) & (df_sf_2017['month'] < end_month)]['space_new']\n",
    "    y_train = df_sf_2017[(df_sf_2017['month'] >= start_month) & (df_sf_2017['month'] < end_month)]['popular']\n",
    "\n",
    "    df_X_test = df_sf_2017[df_sf_2017['month'] == end_month]['space_new']\n",
    "    y_test = df_sf_2017[df_sf_2017['month'] == end_month]['popular']\n",
    "    \n",
    "    return df_X_train, y_train, df_X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_tf_vec(df_X_train, df_X_test):\n",
    "    tf_vectorizer_train = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english').fit(df_X_train)\n",
    "    X_train = tf_vectorizer_train.transform(df_X_train)\n",
    "    tf_vectorizer_test = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', vocabulary = tf_vectorizer_train.vocabulary_).fit(df_X_test)\n",
    "    X_test = tf_vectorizer_test.transform(df_X_test)\n",
    "    return X_train, X_test, tf_vectorizer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<26526x12784 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1848230 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_nb(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    pickle.dump(nb, open('nb_model'+ str(model_num) + '.p', 'wb'))\n",
    "    preds = nb.predict(X_test)\n",
    "    scores_tf_nb[0][model_num] = accuracy_score(y_test, preds)\n",
    "    scores_tf_nb[1][model_num] = recall_score(y_test, nb.predict(X_test))\n",
    "    scores_tf_nb[2][model_num] = precision_score(y_test, nb.predict(X_test))\n",
    "    scores_tf_nb[3][model_num] = f1_score(y_test, nb.predict(X_test))\n",
    "    return scores_tf_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def grid_searching (param_grid, model):\n",
    "\n",
    "#     grid_search = GridSearchCV(model, \n",
    "#                                param_grid=param_grid, cv=5, \n",
    "#                                n_jobs=-1, scoring=make_scorer(f1_score))\n",
    "#     fit = grid_search.fit(X_train, y_train)\n",
    "#     predicted = fit.predict(X_test)\n",
    "#     return grid_search.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the CountVectorizer, run with RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_rf(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators = 500, \n",
    "                                n_jobs = -1, \n",
    "                                random_state = 0, \n",
    "                                max_depth = 10)\n",
    "    rf.fit(X_train, y_train)\n",
    "    predicted = rf.predict(X_test)\n",
    "#     pickle.dump(rf, open('rf_nlp_countvec_50' + str(model_num) + '.p', 'wb'))\n",
    "    scores_tf_rf[0][model_num] = accuracy_score(y_test, predicted)\n",
    "    scores_tf_rf[1][model_num] = recall_score(y_test, predicted)\n",
    "    scores_tf_rf[2][model_num] = precision_score(y_test, predicted)\n",
    "    scores_tf_rf[3][model_num] = f1_score(y_test, predicted)\n",
    "    return scores_tf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tf_nb = np.zeros(shape=(4,9))\n",
    "scores_tf_rf = np.zeros(shape=(4,9))\n",
    "\n",
    "model_num = 0\n",
    "start_month = 1\n",
    "end_month = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 0\n",
      "tf_nb\n",
      "[[0.86741858 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.57871537 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.65549215 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.61471572 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.8200023  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.01700252 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.9        0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.03337454 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "2 5 1\n",
      "tf_nb\n",
      "[[0.86741858 0.86995413 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.57871537 0.57971014 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.65549215 0.66330209 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.61471572 0.61869536 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.8200023  0.8206422  0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.01700252 0.01512287 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.9        0.96       0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.03337454 0.02977667 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "3 6 2\n",
      "tf_nb\n",
      "[[0.86741858 0.86995413 0.86700068 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.57871537 0.57971014 0.57821901 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.65549215 0.66330209 0.67296919 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.61471572 0.61869536 0.62200647 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.8200023  0.8206422  0.81405147 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.01700252 0.01512287 0.01805054 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.9        0.96       0.96774194 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.03337454 0.02977667 0.03544005 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "4 7 3\n",
      "tf_nb\n",
      "[[0.86741858 0.86995413 0.86700068 0.86627447 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.57871537 0.57971014 0.57821901 0.57392317 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.65549215 0.66330209 0.67296919 0.68662953 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.61471572 0.61869536 0.62200647 0.62523779 0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.8200023  0.8206422  0.81405147 0.80982012 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.01700252 0.01512287 0.01805054 0.02211874 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.9        0.96       0.96774194 0.97435897 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.03337454 0.02977667 0.03544005 0.04325555 0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "5 8 4\n",
      "tf_nb\n",
      "[[0.86741858 0.86995413 0.86700068 0.86627447 0.86180571 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.57871537 0.57971014 0.57821901 0.57392317 0.55724362 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.65549215 0.66330209 0.67296919 0.68662953 0.70342466 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.61471572 0.61869536 0.62200647 0.62523779 0.62185892 0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.8200023  0.8206422  0.81405147 0.80982012 0.79973445 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.01700252 0.01512287 0.01805054 0.02211874 0.01899078 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.9        0.96       0.96774194 0.97435897 0.94594595 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.03337454 0.02977667 0.03544005 0.04325555 0.03723404 0.\n",
      "  0.         0.         0.        ]]\n",
      "6 9 5\n",
      "tf_nb\n",
      "[[0.86741858 0.86995413 0.86700068 0.86627447 0.86180571 0.85536365\n",
      "  0.         0.         0.        ]\n",
      " [0.57871537 0.57971014 0.57821901 0.57392317 0.55724362 0.55786667\n",
      "  0.         0.         0.        ]\n",
      " [0.65549215 0.66330209 0.67296919 0.68662953 0.70342466 0.72237569\n",
      "  0.         0.         0.        ]\n",
      " [0.61471572 0.61869536 0.62200647 0.62523779 0.62185892 0.62955161\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.8200023  0.8206422  0.81405147 0.80982012 0.79973445 0.7835742\n",
      "  0.         0.         0.        ]\n",
      " [0.01700252 0.01512287 0.01805054 0.02211874 0.01899078 0.01866667\n",
      "  0.         0.         0.        ]\n",
      " [0.9        0.96       0.96774194 0.97435897 0.94594595 0.94594595\n",
      "  0.         0.         0.        ]\n",
      " [0.03337454 0.02977667 0.03544005 0.04325555 0.03723404 0.03661088\n",
      "  0.         0.         0.        ]]\n",
      "7 10 6\n",
      "tf_nb\n",
      "[[0.86741858 0.86995413 0.86700068 0.86627447 0.86180571 0.85536365\n",
      "  0.85675736 0.         0.        ]\n",
      " [0.57871537 0.57971014 0.57821901 0.57392317 0.55724362 0.55786667\n",
      "  0.56999472 0.         0.        ]\n",
      " [0.65549215 0.66330209 0.67296919 0.68662953 0.70342466 0.72237569\n",
      "  0.70110461 0.         0.        ]\n",
      " [0.61471572 0.61869536 0.62200647 0.62523779 0.62185892 0.62955161\n",
      "  0.62878788 0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.8200023  0.8206422  0.81405147 0.80982012 0.79973445 0.7835742\n",
      "  0.79109512 0.         0.        ]\n",
      " [0.01700252 0.01512287 0.01805054 0.02211874 0.01899078 0.01866667\n",
      "  0.01901743 0.         0.        ]\n",
      " [0.9        0.96       0.96774194 0.97435897 0.94594595 0.94594595\n",
      "  0.97297297 0.         0.        ]\n",
      " [0.03337454 0.02977667 0.03544005 0.04325555 0.03723404 0.03661088\n",
      "  0.0373057  0.         0.        ]]\n",
      "8 11 7\n",
      "tf_nb\n",
      "[[0.86741858 0.86995413 0.86700068 0.86627447 0.86180571 0.85536365\n",
      "  0.85675736 0.84270807 0.        ]\n",
      " [0.57871537 0.57971014 0.57821901 0.57392317 0.55724362 0.55786667\n",
      "  0.56999472 0.58992806 0.        ]\n",
      " [0.65549215 0.66330209 0.67296919 0.68662953 0.70342466 0.72237569\n",
      "  0.70110461 0.70352717 0.        ]\n",
      " [0.61471572 0.61869536 0.62200647 0.62523779 0.62185892 0.62955161\n",
      "  0.62878788 0.64173913 0.        ]]\n",
      "tf_rf\n",
      "[[0.8200023  0.8206422  0.81405147 0.80982012 0.79973445 0.7835742\n",
      "  0.79109512 0.76667091 0.        ]\n",
      " [0.01700252 0.01512287 0.01805054 0.02211874 0.01899078 0.01866667\n",
      "  0.01901743 0.02344791 0.        ]\n",
      " [0.9        0.96       0.96774194 0.97435897 0.94594595 0.94594595\n",
      "  0.97297297 0.97777778 0.        ]\n",
      " [0.03337454 0.02977667 0.03544005 0.04325555 0.03723404 0.03661088\n",
      "  0.0373057  0.04579755 0.        ]]\n",
      "9 12 8\n",
      "tf_nb\n",
      "[[0.86741858 0.86995413 0.86700068 0.86627447 0.86180571 0.85536365\n",
      "  0.85675736 0.84270807 0.82839046]\n",
      " [0.57871537 0.57971014 0.57821901 0.57392317 0.55724362 0.55786667\n",
      "  0.56999472 0.58992806 0.6345727 ]\n",
      " [0.65549215 0.66330209 0.67296919 0.68662953 0.70342466 0.72237569\n",
      "  0.70110461 0.70352717 0.69874733]\n",
      " [0.61471572 0.61869536 0.62200647 0.62523779 0.62185892 0.62955161\n",
      "  0.62878788 0.64173913 0.6651156 ]]\n",
      "tf_rf\n",
      "[[0.8200023  0.8206422  0.81405147 0.80982012 0.79973445 0.7835742\n",
      "  0.79109512 0.76667091 0.73949329]\n",
      " [0.01700252 0.01512287 0.01805054 0.02211874 0.01899078 0.01866667\n",
      "  0.01901743 0.02344791 0.03052164]\n",
      " [0.9        0.96       0.96774194 0.97435897 0.94594595 0.94594595\n",
      "  0.97297297 0.97777778 0.98214286]\n",
      " [0.03337454 0.02977667 0.03544005 0.04325555 0.03723404 0.03661088\n",
      "  0.0373057  0.04579755 0.05920344]]\n",
      "CPU times: user 9min 25s, sys: 416 ms, total: 9min 26s\n",
      "Wall time: 8min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while end_month <13:\n",
    "    df_X_train, y_train, df_X_test, y_test = split_data(start_month, end_month)\n",
    "    X_train, X_test, tf_vectorizer_train = run_tf_vec(df_X_train, df_X_test)\n",
    "    scores_tf_nb = predict_tf_nb(X_train, y_train, X_test, y_test)\n",
    "    scores_tf_rf = predict_tf_rf(X_train, y_train, X_test, y_test)\n",
    "    print(start_month, end_month, model_num)\n",
    "    model_num += 1\n",
    "    start_month += 1\n",
    "    end_month += 1\n",
    "    print('tf_nb')\n",
    "    print(scores_tf_nb)\n",
    "    print('tf_rf')\n",
    "    print(scores_tf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COUNT VECTORIZER NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores_tf_nb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a8e6162883f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy_2017_tf_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_tf_nb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrecall_2017_tf_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_tf_nb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprecision_2017_tf_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_tf_nb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf1_score_2017_tf_nb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_tf_nb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_2017_tf_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores_tf_nb' is not defined"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_nb = np.mean(scores_tf_nb[0])\n",
    "recall_2017_tf_nb = np.mean(scores_tf_nb[1])\n",
    "precision_2017_tf_nb = np.mean(scores_tf_nb[2])\n",
    "f1_score_2017_tf_nb = np.mean(scores_tf_nb[3])\n",
    "print(accuracy_2017_tf_nb)\n",
    "print(recall_2017_tf_nb)\n",
    "print(precision_2017_tf_nb)\n",
    "print(f1_score_2017_tf_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COUNT VECTORIZER RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7938982292282903\n",
      "0.020326566962152987\n",
      "0.9585429344031495\n",
      "0.039777603015805005\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_rf = np.mean(scores_tf_rf[0])\n",
    "recall_2017_tf_rf = np.mean(scores_tf_rf[1])\n",
    "precision_2017_tf_rf = np.mean(scores_tf_rf[2])\n",
    "f1_score_2017_tf_rf = np.mean(scores_tf_rf[3])\n",
    "print(accuracy_2017_tf_rf)\n",
    "print(recall_2017_tf_rf)\n",
    "print(precision_2017_tf_rf)\n",
    "print(f1_score_2017_tf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try running with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tf_idf_vec(df_X_train, df_X_test):\n",
    "    tf_idf_vectorizer_train = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words='english')\n",
    "    X_train = tf_idf_vectorizer_train.fit_transform(df_X_train).toarray()\n",
    "    tf_idf_vectorizer_test = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', vocabulary = tf_idf_vectorizer_train.vocabulary_)\n",
    "    X_test = tf_idf_vectorizer_test.fit_transform(df_X_test).toarray()\n",
    "    return X_train, X_test, tf_idf_vectorizer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf_nb(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    # pickle.dump(nb, open('nb_model_guassian.p', 'wb'))\n",
    "    preds = nb.predict(X_test2)\n",
    "    scores_tf_idf_nb[0][model_num] = accuracy_score(y_test, preds)\n",
    "    scores_tf_idf_nb[1][model_num] = recall_score(y_test, preds)\n",
    "    scores_tf_idf_nb[2][model_num] = precision_score(y_test, preds)\n",
    "    scores_tf_idf_nb[3][model_num] = f1_score(y_test, preds)\n",
    "    return scores_tf_idf_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the TF-IDF, run with RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf_rf(X_train, y_train, X_test, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators = 10, n_jobs=-1, random_state=0, class_weight = {0:.95, 1:.05})\n",
    "    rf.fit(X_train, y_train)\n",
    "    predicted = rf.predict(X_test2)\n",
    "#     pickle.dump(rf, open('rf_nlp_50.p', 'wb'))\n",
    "    scores_tf_idf_rf[0][model_num] = accuracy_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[1][model_num] = recall_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[2][model_num] = precision_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[3][model_num] = f1_score(y_test, predicted)\n",
    "    return scores_tf_idf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = pickle.load(open('rf_nlp_countvec_500.p', 'rb'))\n",
    "# a.fit(X_train, y_train)\n",
    "# preds = a.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tf_idf_nb = np.zeros(shape=(4,9))\n",
    "scores_tf_idf_rf = np.zeros(shape=(4,9))\n",
    "\n",
    "model_num = 0\n",
    "start_month = 1\n",
    "end_month = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 0\n",
      "tf_idf_nb\n",
      "[[0.52894464 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.95906801 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.27436498 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.4266704  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.93336402 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.68576826 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93156544 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.78998912 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "2 5 1\n",
      "tf_idf_nb\n",
      "[[0.52894464 0.52224771 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.95906801 0.94959042 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.27436498 0.26944395 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.4266704  0.41977716 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.93336402 0.92947248 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.68576826 0.66225583 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93156544 0.9300885  0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.78998912 0.77364741 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "3 6 2\n",
      "tf_idf_nb\n",
      "[[0.52894464 0.52224771 0.53085857 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.95906801 0.94959042 0.95607702 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.27436498 0.26944395 0.28193754 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.4266704  0.41977716 0.43546177 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.93336402 0.92947248 0.92894557 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.68576826 0.66225583 0.67509025 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93156544 0.9300885  0.93034826 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.78998912 0.77364741 0.78242678 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "4 7 3\n",
      "tf_idf_nb\n",
      "[[0.52894464 0.52224771 0.53085857 0.53716484 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.95906801 0.94959042 0.95607702 0.95285215 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.27436498 0.26944395 0.28193754 0.28988844 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.4266704  0.41977716 0.43546177 0.44453496 0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.93336402 0.92947248 0.92894557 0.92861183 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.68576826 0.66225583 0.67509025 0.67229336 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93156544 0.9300885  0.93034826 0.94439902 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.78998912 0.77364741 0.78242678 0.78544713 0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "5 8 4\n",
      "tf_idf_nb\n",
      "[[0.52894464 0.52224771 0.53085857 0.53716484 0.53562735 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.95906801 0.94959042 0.95607702 0.95285215 0.93977211 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.27436498 0.26944395 0.28193754 0.28988844 0.2976968  0.\n",
      "  0.         0.         0.        ]\n",
      " [0.4266704  0.41977716 0.43546177 0.44453496 0.45216029 0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.93336402 0.92947248 0.92894557 0.92861183 0.91447223 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.68576826 0.66225583 0.67509025 0.67229336 0.62181226 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93156544 0.9300885  0.93034826 0.94439902 0.93780687 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.78998912 0.77364741 0.78242678 0.78544713 0.74779772 0.\n",
      "  0.         0.         0.        ]]\n",
      "6 9 5\n",
      "tf_idf_nb\n",
      "[[0.52894464 0.52224771 0.53085857 0.53716484 0.53562735 0.55821878\n",
      "  0.         0.         0.        ]\n",
      " [0.95906801 0.94959042 0.95607702 0.95285215 0.93977211 0.96\n",
      "  0.         0.         0.        ]\n",
      " [0.27436498 0.26944395 0.28193754 0.28988844 0.2976968  0.32816773\n",
      "  0.         0.         0.        ]\n",
      " [0.4266704  0.41977716 0.43546177 0.44453496 0.45216029 0.48913043\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.93336402 0.92947248 0.92894557 0.92861183 0.91447223 0.91728352\n",
      "  0.         0.         0.        ]\n",
      " [0.68576826 0.66225583 0.67509025 0.67229336 0.62181226 0.64533333\n",
      "  0.         0.         0.        ]\n",
      " [0.93156544 0.9300885  0.93034826 0.94439902 0.93780687 0.96877502\n",
      "  0.         0.         0.        ]\n",
      " [0.78998912 0.77364741 0.78242678 0.78544713 0.74779772 0.77464789\n",
      "  0.         0.         0.        ]]\n",
      "7 10 6\n",
      "tf_idf_nb\n",
      "[[0.52894464 0.52224771 0.53085857 0.53716484 0.53562735 0.55821878\n",
      "  0.54047673 0.         0.        ]\n",
      " [0.95906801 0.94959042 0.95607702 0.95285215 0.93977211 0.96\n",
      "  0.95298468 0.         0.        ]\n",
      " [0.27436498 0.26944395 0.28193754 0.28988844 0.2976968  0.32816773\n",
      "  0.31092727 0.         0.        ]\n",
      " [0.4266704  0.41977716 0.43546177 0.44453496 0.45216029 0.48913043\n",
      "  0.46887589 0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.93336402 0.92947248 0.92894557 0.92861183 0.91447223 0.91728352\n",
      "  0.91623566 0.         0.        ]\n",
      " [0.68576826 0.66225583 0.67509025 0.67229336 0.62181226 0.64533333\n",
      "  0.65346012 0.         0.        ]\n",
      " [0.93156544 0.9300885  0.93034826 0.94439902 0.93780687 0.96877502\n",
      "  0.93288084 0.         0.        ]\n",
      " [0.78998912 0.77364741 0.78242678 0.78544713 0.74779772 0.77464789\n",
      "  0.76856167 0.         0.        ]]\n",
      "8 11 7\n",
      "tf_idf_nb\n",
      "[[0.52894464 0.52224771 0.53085857 0.53716484 0.53562735 0.55821878\n",
      "  0.54047673 0.57037414 0.        ]\n",
      " [0.95906801 0.94959042 0.95607702 0.95285215 0.93977211 0.96\n",
      "  0.95298468 0.96269651 0.        ]\n",
      " [0.27436498 0.26944395 0.28193754 0.28988844 0.2976968  0.32816773\n",
      "  0.31092727 0.35334963 0.        ]\n",
      " [0.4266704  0.41977716 0.43546177 0.44453496 0.45216029 0.48913043\n",
      "  0.46887589 0.51695522 0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.93336402 0.92947248 0.92894557 0.92861183 0.91447223 0.91728352\n",
      "  0.91623566 0.91632731 0.        ]\n",
      " [0.68576826 0.66225583 0.67509025 0.67229336 0.62181226 0.64533333\n",
      "  0.65346012 0.69704237 0.        ]\n",
      " [0.93156544 0.9300885  0.93034826 0.94439902 0.93780687 0.96877502\n",
      "  0.93288084 0.93629205 0.        ]\n",
      " [0.78998912 0.77364741 0.78242678 0.78544713 0.74779772 0.77464789\n",
      "  0.76856167 0.79914465 0.        ]]\n",
      "9 12 8\n",
      "tf_idf_nb\n",
      "[[0.52894464 0.52224771 0.53085857 0.53716484 0.53562735 0.55821878\n",
      "  0.54047673 0.57037414 0.6033532 ]\n",
      " [0.95906801 0.94959042 0.95607702 0.95285215 0.93977211 0.96\n",
      "  0.95298468 0.96269651 0.96809101]\n",
      " [0.27436498 0.26944395 0.28193754 0.28988844 0.2976968  0.32816773\n",
      "  0.31092727 0.35334963 0.40117282]\n",
      " [0.4266704  0.41977716 0.43546177 0.44453496 0.45216029 0.48913043\n",
      "  0.46887589 0.51695522 0.56727095]]\n",
      "tf_idf_rf\n",
      "[[0.93336402 0.92947248 0.92894557 0.92861183 0.91447223 0.91728352\n",
      "  0.91623566 0.91632731 0.91594635]\n",
      " [0.68576826 0.66225583 0.67509025 0.67229336 0.62181226 0.64533333\n",
      "  0.65346012 0.69704237 0.74112098]\n",
      " [0.93156544 0.9300885  0.93034826 0.94439902 0.93780687 0.96877502\n",
      "  0.93288084 0.93629205 0.93196092]\n",
      " [0.78998912 0.77364741 0.78242678 0.78544713 0.74779772 0.77464789\n",
      "  0.76856167 0.79914465 0.82565688]]\n",
      "CPU times: user 1h 7min 16s, sys: 36.1 s, total: 1h 7min 52s\n",
      "Wall time: 23min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while end_month <13:\n",
    "    df_X_train, y_train, df_X_test, y_test = split_data(start_month, end_month) \n",
    "    X_train2, X_test2, tf_idf_vectorizer_train = run_tf_idf_vec(df_X_train, df_X_test)\n",
    "    scores_tf_idf_nb = predict_tf_idf_nb(X_train2, y_train, X_test2, y_test)\n",
    "    scores_tf_idf_rf = predict_tf_idf_rf(X_train2, y_train, X_test2, y_test)\n",
    "    print(start_month, end_month, model_num)\n",
    "    model_num += 1\n",
    "    start_month += 1\n",
    "    end_month += 1\n",
    "    print('tf_idf_nb')\n",
    "    print(scores_tf_idf_nb)\n",
    "    print('tf_idf_rf')\n",
    "    print(scores_tf_idf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5474739954473133\n",
      "0.9556813235672457\n",
      "0.31188323987706895\n",
      "0.46898189814778557\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[0])\n",
    "recall_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[1])\n",
    "precision_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[2])\n",
    "f1_score_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[3])\n",
    "print(accuracy_2017_tf_idf_nb)\n",
    "print(recall_2017_tf_idf_nb)\n",
    "print(precision_2017_tf_idf_nb)\n",
    "print(f1_score_2017_tf_idf_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9222954413043342\n",
      "0.6726863069580991\n",
      "0.938235214202308\n",
      "0.7830354690880177\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[0])\n",
    "recall_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[1])\n",
    "precision_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[2])\n",
    "f1_score_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[3])\n",
    "print(accuracy_2017_tf_idf_rf)\n",
    "print(recall_2017_tf_idf_rf)\n",
    "print(precision_2017_tf_idf_rf)\n",
    "print(f1_score_2017_tf_idf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I need k means? Or just top twenty words for popular and top twenty words for not popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  tf_idf_vectorizer_train.get_feature_names()\n",
    "\n",
    "start_month = 1\n",
    "end_month = 4\n",
    "df_X_train = df_sf_2017[(df_sf_2017['month'] >= start_month) & (df_sf_2017['month'] < end_month)]['space_new']\n",
    "y_train = df_sf_2017[(df_sf_2017['month'] >= start_month) & (df_sf_2017['month'] < end_month)]['popular']\n",
    "\n",
    "df_X_test = df_sf_2017[df_sf_2017['month'] == end_month]['space_new']\n",
    "y_test = df_sf_2017[df_sf_2017['month'] == end_month]['popular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1300',\n",
       " '1300sqft',\n",
       " '1304',\n",
       " '1315',\n",
       " '132',\n",
       " '135',\n",
       " '1350',\n",
       " '135cm',\n",
       " '136',\n",
       " '137',\n",
       " '1370123',\n",
       " '13724120',\n",
       " '139',\n",
       " '1394',\n",
       " '1396',\n",
       " '13ft',\n",
       " '13in',\n",
       " '13lb',\n",
       " '13mn',\n",
       " '13th',\n",
       " '13x12',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '1400ft²',\n",
       " '1400sf',\n",
       " '1408',\n",
       " '140sq',\n",
       " '140sqm',\n",
       " '14154147',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '1450',\n",
       " '14611926',\n",
       " '1474',\n",
       " '149',\n",
       " '14ft',\n",
       " '14l',\n",
       " '14min',\n",
       " '14r',\n",
       " '14sqm',\n",
       " '14th',\n",
       " '14x',\n",
       " '14x12',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '1500ft',\n",
       " '1500sf']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[150:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 3.51 s, total: 1min 20s\n",
      "Wall time: 1min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = KMeans(n_clusters=2, n_jobs=-1)\n",
    "kmeans.fit(X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features (words) for each cluster:\n",
      "0: san, apartment, block, francisco, street, place, restaurant, neighborhood, park, kitchen, room, city, bedroom, walk, located, access, home, close, great, bed, ha, away, private, view, location, mission, space, sf, minute, 2, walking, bathroom, downtown, area, parking, house, bar, floor, distance, bus, bart, quiet, gate, easy, guest, available, district, public, golden, building, stay, 1, square, shop, just, unit, heart, youll, beautiful\n",
      "1: room, bedroom, bed, kitchen, ha, bathroom, living, private, home, san, apartment, large, 2, access, francisco, house, floor, guest, street, queen, block, space, park, area, neighborhood, located, view, 1, city, restaurant, great, tv, parking, dining, quiet, walk, spacious, available, bath, shared, away, flat, beautiful, comfortable, sf, mission, garden, place, stay, size, 3, downtown, open, deck, easy, just, unit, close, new\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF vectorizer\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-60:-1]\n",
    "print(\"top features (words) for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(features[i] for i in centroid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features (words) for each cluster:\n",
      "0: space, home, house, city, san, place, great, view, francisco, apartment, room, location, quiet, light, sf, floor, comfortable, ha, neighborhood, lot, close, restaurant, clean, unit, street, guest, beautiful, time, studio, park, modern, building, stay, feel, wa, like, kitchen, perfect, people, cozy, art, need, block, bed, unique, new, located, available, just\n",
      "1: room, bedroom, bed, ha, kitchen, bathroom, living, private, apartment, large, floor, space, san, queen, home, 2, francisco, house, street, area, guest, tv, view, block, access, size, great, located, 1, city, comfortable, neighborhood, dining, park, spacious, bath, parking, flat, new, unit, quiet, away, closet, garden, window, walk, beautiful, coffee, available\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 2.92 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF-IDF vectorizer\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-50:-1]\n",
    "print(\"top features (words) for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(features[i] for i in centroid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catPred(kmObj, gTrue, pred): \n",
    "    '''\n",
    "        kmObjn: a kmeans object\n",
    "        gTrue: true categories (ground truth)\n",
    "    '''\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True, figsize=(50,12))\n",
    "\n",
    "    numClusters = kmObj.get_params()['n_clusters']\n",
    "    \n",
    "    for i in range(numClusters):\n",
    "        print(i)\n",
    "        mask = (pred == i)\n",
    "        val = np.unique(gTrue[mask], return_counts=True)\n",
    "        lbl = val[0]\n",
    "        ht  = val[1] / val[1].sum()\n",
    "        ax[i].bar(range(numClusters), height=ht, tick_label = lbl)\n",
    "        ax[i].set_xticklabels(lbl, fontsize=20)\n",
    "        ax[i].set_title(i, fontsize=20)\n",
    "\n",
    "        ax[0].set_yticklabels([\"0\", \"0.2\", \"0.4\", \"0.6\", \"0.8\"], fontsize=20)\n",
    "        ax[0].set_ylabel(\"Proportion\", fontsize = 32);\n",
    "        plt.suptitle(\"Proportion of Categories in each Cluster\", fontsize = 32);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5afe2d70811e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcatPred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "catPred(kmeans, y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.get_params()['n_clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
