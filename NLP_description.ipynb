{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime as dt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\",999)\n",
    "pd.set_option(\"display.max_rows\",999)\n",
    "pd.set_option(\"display.max_columns\",999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017 = pickle.load(open('data_sf_2017.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103956"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sf_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017.description.fillna(value='None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.68 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def process_text(text):\n",
    "    from nltk.tokenize import RegexpTokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    stemmed = []\n",
    "    snowball = SnowballStemmer(\"english\")\n",
    "    for item in tokens:\n",
    "        stemmed.append(snowball.stem(item))\n",
    "        \n",
    "    lemmatized = []\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    for item in stemmed:\n",
    "        lemmatized.append(wordnet.lemmatize(item))\n",
    "    \n",
    "    return lemmatized\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data - running NLP on description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_train = df_sf_2017[(df_sf_2017['month'] >= 1) & (df_sf_2017['month'] < 4)]['description']\n",
    "y_train = df_sf_2017[(df_sf_2017['month'] >= 1) & (df_sf_2017['month'] < 4)]['popular']\n",
    "\n",
    "df_X_test = df_sf_2017[df_sf_2017['month'] == 4]['description']\n",
    "y_test = df_sf_2017[df_sf_2017['month'] == 4]['popular']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 56s, sys: 68 ms, total: 1min 56s\n",
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf_vectorizer_train = CountVectorizer(analyzer=process_text).fit(df_X_train)\n",
    "X_train = tf_vectorizer_train.transform(df_X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.3 s, sys: 0 ns, total: 38.3 s\n",
      "Wall time: 38.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_vectorizer_test = CountVectorizer(analyzer=process_text, vocabulary=tf_vectorizer_train.vocabulary_).fit(df_X_test)\n",
    "X_test = tf_vectorizer_test.transform(df_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vect = CountVectorizer(stop_words='english',lowercase=True, ngram_range=(1,2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=<function process_text at 0x7f57a9f9a510>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=None, strip_accents=None,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8700, 12804)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26530, 12804)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8544827586206897\n",
      "0.663727959697733\n",
      "0.5901455767077267\n",
      "0.6247777119146414\n"
     ]
    }
   ],
   "source": [
    "# def predict_data(X_train, y_train, X_test, y_test):\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "# pickle.dump(nb, open('nb_model.p', 'wb'))\n",
    "preds = nb.predict(X_test)\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(recall_score(y_test, nb.predict(X_test)))\n",
    "print(precision_score(y_test, nb.predict(X_test)))\n",
    "print(f1_score(y_test, nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read pickle file on multinomialNB if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb = pickle.load(open('nb_model.p', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the CountVectorizer, run with RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9570114942528736\n",
      "0.8035264483627204\n",
      "0.953662182361734\n",
      "0.8721804511278195\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 50, random_state=0, class_weight = {0:.95, 1:.05})\n",
    "rf.fit(X_train, y_train)\n",
    "predicted = rf.predict(X_test)\n",
    "pickle.dump(rf, open('rf_nlp_countvec_50.p', 'wb'))\n",
    "print(accuracy_score(y_test, predicted))\n",
    "print(recall_score(y_test, rf.predict(X_test)))\n",
    "print(precision_score(y_test, rf.predict(X_test)))\n",
    "print(f1_score(y_test, rf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try running with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.4 s, sys: 528 ms, total: 59 s\n",
      "Wall time: 59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_idf_vectorizer_train = TfidfVectorizer(analyzer=process_text)\n",
    "X_train2 = tf_idf_vectorizer_train.fit_transform(df_X_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.3 s, sys: 192 ms, total: 19.5 s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tf_idf_vectorizer_test = TfidfVectorizer(analyzer=process_text, vocabulary=tf_idf_vectorizer_train.vocabulary_)\n",
    "X_test2 = tf_idf_vectorizer_test.fit_transform(df_X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8700, 12804)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26530, 12804)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6135632183908046\n",
      "0.9370277078085643\n",
      "0.31326315789473685\n",
      "0.4695487535500158\n"
     ]
    }
   ],
   "source": [
    "# def predict_data(X_train, y_train, X_test, y_test):\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train2, y_train)\n",
    "# pickle.dump(nb, open('nb_model_guassian.p', 'wb'))\n",
    "preds = nb.predict(X_test2)\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(recall_score(y_test, nb.predict(X_test2)))\n",
    "print(precision_score(y_test, nb.predict(X_test2)))\n",
    "print(f1_score(y_test, nb.predict(X_test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Shape of Sparse Matrix: ', X.shape)\n",
    "# print('Amount of Non-Zero occurrences: ', X.nnz)\n",
    "# # Percentage of non-zero values\n",
    "# density = (100.0 * X.nnz / (X.shape[0] * X.shape[1]))\n",
    "# print('Density: {}'.format((density)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the TF-IDF, run with RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 200, random_state=0, class_weight = {0:.95, 1:.05})\n",
    "rf.fit(X_train2, y_train)\n",
    "predicted = rf.predict(X_test2)\n",
    "pickle.dump(rf, open('rf_nlp_200.p', 'wb'))\n",
    "print(accuracy_score(y_test, predicted))\n",
    "print(recall_score(y_test, rf.predict(X_test2)))\n",
    "print(precision_score(y_test, rf.predict(X_test2)))\n",
    "print(f1_score(y_test, rf.predict(X_test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
