{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline score -  randomly guessing given that we know that 20% of the data is popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime as dt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\",999)\n",
    "pd.set_option(\"display.max_rows\",999)\n",
    "pd.set_option(\"display.max_columns\",999)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_guessing(n_simulations = 10):\n",
    "    \n",
    "    count = 0 #\n",
    "    #keep track of \n",
    "    accuracy_baseline = np.zeros(shape=(1,9*n_simulations)) #we have 9 test sets for 2017. Multiply by 9 for each simulation.  \n",
    "    recall_baseline = np.zeros(shape=(1,9*n_simulations))\n",
    "    precision_baseline = np.zeros(shape=(1,9*n_simulations))\n",
    "    f1_baseline = np.zeros(shape=(1,9*n_simulations))\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        my_list = [True for x in range(2)] +  [False for x in range(8)]\n",
    "        start_month = 1\n",
    "        end_month = 4\n",
    "        while end_month <13:\n",
    "            y_test = df_sf_2017[df_sf_2017['month'] == end_month]['popular']\n",
    "            y_pred = pd.Series(random.choice(my_list) for x in range(y_test.size))\n",
    "            accuracy_baseline[0][count] = accuracy_score(y_test, y_pred)\n",
    "            recall_baseline[0][count] = recall_score(y_test, y_pred)\n",
    "            precision_baseline[0][count] = precision_score(y_test, y_pred)\n",
    "            f1_baseline[0][count] = f1_score(y_test, y_pred)\n",
    "            count+=1\n",
    "            start_month += 1\n",
    "            end_month += 1\n",
    "            \n",
    "    return accuracy_baseline, recall_baseline, precision_baseline, f1_baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_baseline, recall_baseline, precision_baseline, f1_baseline = randomly_guessing(n_simulations = 1000)\n",
    "print(accuracy_baseline.mean())\n",
    "print(recall_baseline.mean()) \n",
    "print(precision_baseline.mean()) \n",
    "print(f1_baseline.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sf_2017 = pickle.load(open('../data_sf_2017.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017 = pd.read_json('df_sf_2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117262"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sf_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017.description.fillna(value='None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017[\"description_new\"] = df_sf_2017['description'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = df_sf_2017['description'].iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string \n",
    "# def remove_punctuations(text):\n",
    "#     return text.translate(None,string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punc = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_pattern = r'\\w+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = df_sf_2017['description_new'].iloc[1:3]\n",
    "tf_vectorizer_train = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', token_pattern = r'\\w+').fit(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'floor': 30,\n",
       " 'breath': 15,\n",
       " 'taking': 86,\n",
       " 'view': 96,\n",
       " 'easy': 25,\n",
       " 'transportation': 91,\n",
       " 'downtown': 24,\n",
       " 'great': 35,\n",
       " 'neighborhood': 55,\n",
       " 'san': 76,\n",
       " 'francisco': 31,\n",
       " 'safe': 75,\n",
       " 'free': 32,\n",
       " 'parking': 59,\n",
       " 'located': 46,\n",
       " 'geographically': 34,\n",
       " 'center': 17,\n",
       " 'city': 18,\n",
       " 'public': 68,\n",
       " 'available': 10,\n",
       " '15': 1,\n",
       " 'minute': 51,\n",
       " 'private': 67,\n",
       " 'bathroom': 11,\n",
       " 'room': 74,\n",
       " 'street': 83,\n",
       " 'lovely': 48,\n",
       " 'noe': 57,\n",
       " 'valley': 94,\n",
       " 'access': 5,\n",
       " 'muni': 54,\n",
       " 'market': 49,\n",
       " 'perfectly': 63,\n",
       " 'enjoy': 26,\n",
       " 'peaceful': 61,\n",
       " 'environment': 28,\n",
       " 'want': 97,\n",
       " 'close': 20,\n",
       " 'shopping': 81,\n",
       " 'restaurant': 72,\n",
       " 'nightlife': 56,\n",
       " 'block': 14,\n",
       " 'j': 41,\n",
       " 'train': 90,\n",
       " '20m': 3,\n",
       " 'ride': 73,\n",
       " '3': 4,\n",
       " 'mission': 52,\n",
       " 'apartment': 8,\n",
       " '2': 2,\n",
       " 'bedroom': 12,\n",
       " '1': 0,\n",
       " 'recently': 70,\n",
       " 'remodeled': 71,\n",
       " 'modern': 53,\n",
       " 'kitchen': 42,\n",
       " 'dishwasher': 23,\n",
       " 'washerdryer': 98,\n",
       " 'building': 16,\n",
       " 'use': 93,\n",
       " 'best': 13,\n",
       " 'hill': 38,\n",
       " 'climb': 19,\n",
       " 'sunny': 85,\n",
       " 'airy': 6,\n",
       " 'minimalist': 50,\n",
       " 'terrarium': 87,\n",
       " 'various': 95,\n",
       " 'plant': 66,\n",
       " 'scattered': 78,\n",
       " 'entire': 27,\n",
       " 'place': 65,\n",
       " 'including': 40,\n",
       " 'living': 45,\n",
       " 'dining': 22,\n",
       " 'garage': 33,\n",
       " 'love': 47,\n",
       " 'spend': 82,\n",
       " 'time': 88,\n",
       " 'guest': 36,\n",
       " 'listed': 44,\n",
       " 'im': 39,\n",
       " 'town': 89,\n",
       " 'reachable': 69,\n",
       " 'phone': 64,\n",
       " 'anytime': 7,\n",
       " 'offer': 58,\n",
       " 'suggestionsadvice': 84,\n",
       " 'arrive': 9,\n",
       " 'favorite': 29,\n",
       " 'share': 80,\n",
       " 'truly': 92,\n",
       " 'sense': 79,\n",
       " 'community': 21,\n",
       " 'people': 62,\n",
       " 'like': 43,\n",
       " 'say': 77,\n",
       " 'hello': 37,\n",
       " 'passing': 60}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer_train.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['walk', 'hippo', 'gees', 'gees', 'goos', 'walk']\n"
     ]
    }
   ],
   "source": [
    "text = 'walk you i san are to .be hippo geese. geese goose francisco walked'\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.append('san')\n",
    "stop_words.append('francisco')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "stemmed = []\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "for item in tokens:\n",
    "    if item not in stop_words:\n",
    "        stemmed.append(snowball.stem(item))\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'san',\n",
       " 'francisco']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# def process_text(text):\n",
    "#     stop_words = stopwords.words(\"english\")\n",
    "#     stop_words.append('san')\n",
    "#     stop_words.append('francisco')\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "#     stemmed = []\n",
    "#     snowball = SnowballStemmer(\"english\")\n",
    "#     for item in tokens:\n",
    "#         if item not in stop_words:\n",
    "#             stemmed.append(snowball.stem(item))\n",
    "        \n",
    "#     lemmatized = []\n",
    "#     wordnet = WordNetLemmatizer()\n",
    "#     for item in stemmed:\n",
    "#         lemmatized.append(wordnet.lemmatize(item))\n",
    "    \n",
    "#     return lemmatized\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LemmaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data - running NLP on description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(start_month, end_month):\n",
    "    df_X_train = df_sf_2017[(df_sf_2017['month'] >= start_month) & (df_sf_2017['month'] < end_month)]['description_new']\n",
    "    y_train = df_sf_2017[(df_sf_2017['month'] >= start_month) & (df_sf_2017['month'] < end_month)]['popular']\n",
    "\n",
    "    df_X_test = df_sf_2017[df_sf_2017['month'] == end_month]['description_new']\n",
    "    y_test = df_sf_2017[df_sf_2017['month'] == end_month]['popular']\n",
    "    \n",
    "    return df_X_train, y_train, df_X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_tf_vec(df_X_train, df_X_test):\n",
    "    tf_vectorizer_train = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english').fit(df_X_train)\n",
    "    X_train = tf_vectorizer_train.transform(df_X_train)\n",
    "    tf_vectorizer_test = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', vocabulary = tf_vectorizer_train.vocabulary_).fit(df_X_test)\n",
    "    X_test = tf_vectorizer_test.transform(df_X_test)\n",
    "    return X_train, X_test, tf_vectorizer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<26526x12784 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1848230 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_nb(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    pickle.dump(nb, open('nb_model'+ str(model_num) + '.p', 'wb'))\n",
    "    preds = nb.predict(X_test)\n",
    "    scores_tf_nb[0][model_num] = accuracy_score(y_test, preds)\n",
    "    scores_tf_nb[1][model_num] = recall_score(y_test, nb.predict(X_test))\n",
    "    scores_tf_nb[2][model_num] = precision_score(y_test, nb.predict(X_test))\n",
    "    scores_tf_nb[3][model_num] = f1_score(y_test, nb.predict(X_test))\n",
    "    return scores_tf_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the CountVectorizer, run with RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_rf(X_train, y_train, X_test, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators = 10, n_jobs = -1, random_state=0, class_weight = {0:.95, 1:.05})\n",
    "    rf.fit(X_train, y_train)\n",
    "    predicted = rf.predict(X_test)\n",
    "    pickle.dump(rf, open('rf_nlp_countvec_50' + str(model_num) + '.p', 'wb'))\n",
    "    scores_tf_rf[0][model_num] = accuracy_score(y_test, predicted)\n",
    "    scores_tf_rf[1][model_num] = recall_score(y_test, predicted)\n",
    "    scores_tf_rf[2][model_num] = precision_score(y_test, predicted)\n",
    "    scores_tf_rf[3][model_num] = f1_score(y_test, predicted)\n",
    "    return scores_tf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tf_nb = np.zeros(shape=(4,9))\n",
    "scores_tf_rf = np.zeros(shape=(4,9))\n",
    "\n",
    "model_num = 0\n",
    "start_month = 1\n",
    "end_month = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 0\n",
      "tf_nb\n",
      "[[0.88228532 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.70465995 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.66845878 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.68608216 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.95574204 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.81171285 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93745455 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.87006412 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "2 5 1\n",
      "tf_nb\n",
      "[[0.88228532 0.88582226 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.70465995 0.71770636 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.66845878 0.67476303 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.68608216 0.69557252 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.95574204 0.95419148 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.81171285 0.78197858 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93745455 0.95830116 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.87006412 0.86120749 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "3 6 2\n",
      "tf_nb\n",
      "[[0.88228532 0.88582226 0.88552916 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.70465995 0.71770636 0.71153846 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.66845878 0.67476303 0.69199299 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.68608216 0.69557252 0.70162963 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.95574204 0.95419148 0.95168808 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.81171285 0.78197858 0.78425481 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93745455 0.95830116 0.95185996 0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.87006412 0.86120749 0.85996705 0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "4 7 3\n",
      "tf_nb\n",
      "[[0.88228532 0.88582226 0.88552916 0.88285133 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.70465995 0.71770636 0.71153846 0.71187427 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.66845878 0.67476303 0.69199299 0.69291785 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.68608216 0.69557252 0.70162963 0.70226816 0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.95574204 0.95419148 0.95168808 0.94961591 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.81171285 0.78197858 0.78425481 0.77764843 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93745455 0.95830116 0.95185996 0.95428571 0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.87006412 0.86120749 0.85996705 0.85695959 0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "5 8 4\n",
      "tf_nb\n",
      "[[0.88228532 0.88582226 0.88552916 0.88285133 0.87767956 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.70465995 0.71770636 0.71153846 0.71187427 0.68763557 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.66845878 0.67476303 0.69199299 0.69291785 0.70483602 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.68608216 0.69557252 0.70162963 0.70226816 0.69612956 0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.95574204 0.95419148 0.95168808 0.94961591 0.94187845 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.81171285 0.78197858 0.78425481 0.77764843 0.75488069 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93745455 0.95830116 0.95185996 0.95428571 0.94952251 0.\n",
      "  0.         0.         0.        ]\n",
      " [0.87006412 0.86120749 0.85996705 0.85695959 0.84108761 0.\n",
      "  0.         0.         0.        ]]\n",
      "6 9 5\n",
      "tf_nb\n",
      "[[0.88228532 0.88582226 0.88552916 0.88285133 0.87767956 0.8695142\n",
      "  0.         0.         0.        ]\n",
      " [0.70465995 0.71770636 0.71153846 0.71187427 0.68763557 0.69013333\n",
      "  0.         0.         0.        ]\n",
      " [0.66845878 0.67476303 0.69199299 0.69291785 0.70483602 0.7090411\n",
      "  0.         0.         0.        ]\n",
      " [0.68608216 0.69557252 0.70162963 0.70226816 0.69612956 0.69945946\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.95574204 0.95419148 0.95168808 0.94961591 0.94187845 0.94578737\n",
      "  0.         0.         0.        ]\n",
      " [0.81171285 0.78197858 0.78425481 0.77764843 0.75488069 0.7792\n",
      "  0.         0.         0.        ]\n",
      " [0.93745455 0.95830116 0.95185996 0.95428571 0.94952251 0.96819085\n",
      "  0.         0.         0.        ]\n",
      " [0.87006412 0.86120749 0.85996705 0.85695959 0.84108761 0.86347518\n",
      "  0.         0.         0.        ]]\n",
      "7 10 6\n",
      "tf_nb\n",
      "[[0.88228532 0.88582226 0.88552916 0.88285133 0.87767956 0.8695142\n",
      "  0.87331536 0.         0.        ]\n",
      " [0.70465995 0.71770636 0.71153846 0.71187427 0.68763557 0.69013333\n",
      "  0.70470153 0.         0.        ]\n",
      " [0.66845878 0.67476303 0.69199299 0.69291785 0.70483602 0.7090411\n",
      "  0.70099842 0.         0.        ]\n",
      " [0.68608216 0.69557252 0.70162963 0.70226816 0.69612956 0.69945946\n",
      "  0.7028451  0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.95574204 0.95419148 0.95168808 0.94961591 0.94187845 0.94578737\n",
      "  0.94047619 0.         0.        ]\n",
      " [0.81171285 0.78197858 0.78425481 0.77764843 0.75488069 0.7792\n",
      "  0.75699947 0.         0.        ]\n",
      " [0.93745455 0.95830116 0.95185996 0.95428571 0.94952251 0.96819085\n",
      "  0.95342648 0.         0.        ]\n",
      " [0.87006412 0.86120749 0.85996705 0.85695959 0.84108761 0.86347518\n",
      "  0.84393404 0.         0.        ]]\n",
      "8 11 7\n",
      "tf_nb\n",
      "[[0.88228532 0.88582226 0.88552916 0.88285133 0.87767956 0.8695142\n",
      "  0.87331536 0.86222674 0.        ]\n",
      " [0.70465995 0.71770636 0.71153846 0.71187427 0.68763557 0.69013333\n",
      "  0.70470153 0.73621103 0.        ]\n",
      " [0.66845878 0.67476303 0.69199299 0.69291785 0.70483602 0.7090411\n",
      "  0.70099842 0.70109109 0.        ]\n",
      " [0.68608216 0.69557252 0.70162963 0.70226816 0.69612956 0.69945946\n",
      "  0.7028451  0.71822199 0.        ]]\n",
      "tf_rf\n",
      "[[0.95574204 0.95419148 0.95168808 0.94961591 0.94187845 0.94578737\n",
      "  0.94047619 0.94312405 0.        ]\n",
      " [0.81171285 0.78197858 0.78425481 0.77764843 0.75488069 0.7792\n",
      "  0.75699947 0.81055156 0.        ]\n",
      " [0.93745455 0.95830116 0.95185996 0.95428571 0.94952251 0.96819085\n",
      "  0.95342648 0.94296342 0.        ]\n",
      " [0.87006412 0.86120749 0.85996705 0.85695959 0.84108761 0.86347518\n",
      "  0.84393404 0.87175813 0.        ]]\n",
      "9 12 8\n",
      "tf_nb\n",
      "[[0.88228532 0.88582226 0.88552916 0.88285133 0.87767956 0.8695142\n",
      "  0.87331536 0.86222674 0.84944957]\n",
      " [0.70465995 0.71770636 0.71153846 0.71187427 0.68763557 0.69013333\n",
      "  0.70470153 0.73621103 0.78773585]\n",
      " [0.66845878 0.67476303 0.69199299 0.69291785 0.70483602 0.7090411\n",
      "  0.70099842 0.70109109 0.69277696]\n",
      " [0.68608216 0.69557252 0.70162963 0.70226816 0.69612956 0.69945946\n",
      "  0.7028451  0.71822199 0.73721111]]\n",
      "tf_rf\n",
      "[[0.95574204 0.95419148 0.95168808 0.94961591 0.94187845 0.94578737\n",
      "  0.94047619 0.94312405 0.94480809]\n",
      " [0.81171285 0.78197858 0.78425481 0.77764843 0.75488069 0.7792\n",
      "  0.75699947 0.81055156 0.85960044]\n",
      " [0.93745455 0.95830116 0.95185996 0.95428571 0.94952251 0.96819085\n",
      "  0.95342648 0.94296342 0.92921416]\n",
      " [0.87006412 0.86120749 0.85996705 0.85695959 0.84108761 0.86347518\n",
      "  0.84393404 0.87175813 0.89305275]]\n",
      "CPU times: user 15min 43s, sys: 168 ms, total: 15min 43s\n",
      "Wall time: 14min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while end_month <13:\n",
    "    df_X_train, y_train, df_X_test, y_test = split_data(start_month, end_month)\n",
    "    X_train, X_test, tf_vectorizer_train = run_tf_vec(df_X_train, df_X_test)\n",
    "    scores_tf_nb = predict_tf_nb(X_train, y_train, X_test, y_test)\n",
    "    scores_tf_rf = predict_tf_rf(X_train, y_train, X_test, y_test)\n",
    "    print(start_month, end_month, model_num)\n",
    "    model_num += 1\n",
    "    start_month += 1\n",
    "    end_month += 1\n",
    "    print('tf_nb')\n",
    "    print(scores_tf_nb)\n",
    "    print('tf_rf')\n",
    "    print(scores_tf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8742970560045102\n",
      "0.7169107075713074\n",
      "0.6929862495075171\n",
      "0.7043799660889926\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_nb = np.mean(scores_tf_nb[0])\n",
    "recall_2017_tf_nb = np.mean(scores_tf_nb[1])\n",
    "precision_2017_tf_nb = np.mean(scores_tf_nb[2])\n",
    "f1_score_2017_tf_nb = np.mean(scores_tf_nb[3])\n",
    "print(accuracy_2017_tf_nb)\n",
    "print(recall_2017_tf_nb)\n",
    "print(precision_2017_tf_nb)\n",
    "print(f1_score_2017_tf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9474790730435897\n",
      "0.790758536328864\n",
      "0.9494687554573628\n",
      "0.8623895527424829\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_rf = np.mean(scores_tf_rf[0])\n",
    "recall_2017_tf_rf = np.mean(scores_tf_rf[1])\n",
    "precision_2017_tf_rf = np.mean(scores_tf_rf[2])\n",
    "f1_score_2017_tf_rf = np.mean(scores_tf_rf[3])\n",
    "print(accuracy_2017_tf_rf)\n",
    "print(recall_2017_tf_rf)\n",
    "print(precision_2017_tf_rf)\n",
    "print(f1_score_2017_tf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try running with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_tf_idf_vec(df_X_train, df_X_test):\n",
    "    tf_idf_vectorizer_train = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words='english')\n",
    "    X_train2 = tf_idf_vectorizer_train.fit_transform(df_X_train).toarray()\n",
    "    tf_idf_vectorizer_test = TfidfVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', vocabulary = tf_vectorizer_train.vocabulary_)\n",
    "    X_test2 = tf_idf_vectorizer_test.fit_transform(df_X_test).toarray()\n",
    "    return X_train2, X_test2, tf_idf_vectorizer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf_nb(X_train2, y_train, X_test2, y_test):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train2, y_train)\n",
    "    # pickle.dump(nb, open('nb_model_guassian.p', 'wb'))\n",
    "    preds = nb.predict(X_test2)\n",
    "    scores_tf_idf_nb[0][model_num] = accuracy_score(y_test, preds)\n",
    "    scores_tf_idf_nb[1][model_num] = recall_score(y_test, preds)\n",
    "    scores_tf_idf_nb[2][model_num] = precision_score(y_test, preds)\n",
    "    scores_tf_idf_nb[3][model_num] = f1_score(y_test, preds)\n",
    "    return scores_tf_idf_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the TF-IDF, run with RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf_rf(X_train2, y_train, X_test2, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators = 10, n_jobs=-1, random_state=0, class_weight = {0:.95, 1:.05})\n",
    "    rf.fit(X_train2, y_train)\n",
    "    predicted = rf.predict(X_test2)\n",
    "#     pickle.dump(rf, open('rf_nlp_50.p', 'wb'))\n",
    "    scores_tf_idf_rf[0][model_num] = accuracy_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[1][model_num] = recall_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[2][model_num] = precision_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[3][model_num] = f1_score(y_test, predicted)\n",
    "    return scores_tf_idf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tf_idf_nb = np.zeros(shape=(4,9))\n",
    "scores_tf_idf_rf = np.zeros(shape=(4,9))\n",
    "\n",
    "model_num = 0\n",
    "start_month = 1\n",
    "end_month = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 0\n",
      "tf_idf_nb\n",
      "[[0.61328888 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93702771 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.31313131 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.46940063 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.94643062 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.76448363 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.9295559  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.83897719 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "CPU times: user 6min 52s, sys: 3.94 s, total: 6min 56s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while end_month <13:\n",
    "    df_X_train, y_train, df_X_test, y_test = split_data(start_month, end_month) \n",
    "    X_train2, X_test2, tf_idf_vectorizer_train = run_tf_idf_vec(df_X_train, df_X_test)\n",
    "    scores_tf_idf_nb = predict_tf_idf_nb(X_train2, y_train, X_test2, y_test)\n",
    "    scores_tf_idf_rf = predict_tf_idf_rf(X_train2, y_train, X_test2, y_test)\n",
    "    print(start_month, end_month, model_num)\n",
    "    model_num += 1\n",
    "    start_month += 1\n",
    "    end_month += 1\n",
    "    print('tf_idf_nb')\n",
    "    print(scores_tf_idf_nb)\n",
    "    print('tf_idf_rf')\n",
    "    print(scores_tf_idf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6223984688066541\n",
      "0.9360869500755025\n",
      "0.3467272958014258\n",
      "0.5052056786500755\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[0])\n",
    "recall_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[1])\n",
    "precision_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[2])\n",
    "f1_score_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[3])\n",
    "print(accuracy_2017_tf_idf_nb)\n",
    "print(recall_2017_tf_idf_nb)\n",
    "print(precision_2017_tf_idf_nb)\n",
    "print(f1_score_2017_tf_idf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9383264864764923\n",
      "0.7493591861607887\n",
      "0.9398408080684738\n",
      "0.8334356589996533\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[0])\n",
    "recall_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[1])\n",
    "precision_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[2])\n",
    "f1_score_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[3])\n",
    "print(accuracy_2017_tf_idf_rf)\n",
    "print(recall_2017_tf_idf_rf)\n",
    "print(precision_2017_tf_idf_rf)\n",
    "print(f1_score_2017_tf_idf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I need k means? Or just top twenty words for popular and top twenty words for not popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  tf_vectorizer_train.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1300',\n",
       " '1300sqft',\n",
       " '1304',\n",
       " '1315',\n",
       " '132',\n",
       " '135',\n",
       " '1350',\n",
       " '135cm',\n",
       " '136',\n",
       " '137',\n",
       " '1370123',\n",
       " '13724120',\n",
       " '139',\n",
       " '1394',\n",
       " '1396',\n",
       " '13ft',\n",
       " '13in',\n",
       " '13lb',\n",
       " '13mn',\n",
       " '13th',\n",
       " '13x12',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '1400ft²',\n",
       " '1400sf',\n",
       " '1408',\n",
       " '140sq',\n",
       " '140sqm',\n",
       " '14154147',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '1450',\n",
       " '14611926',\n",
       " '1474',\n",
       " '149',\n",
       " '14ft',\n",
       " '14l',\n",
       " '14min',\n",
       " '14r',\n",
       " '14sqm',\n",
       " '14th',\n",
       " '14x',\n",
       " '14x12',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '1500ft',\n",
       " '1500sf']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[150:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 988 ms, sys: 0 ns, total: 988 ms\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = KMeans(n_clusters=2, n_jobs=-1)\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features (words) for each cluster:\n",
      "0: san, apartment, block, francisco, street, place, restaurant, neighborhood, park, kitchen, room, city, bedroom, walk, located, access, home, close, great, bed, ha, away, private, view, location, mission, space, sf, minute, 2, walking, bathroom, downtown, area, parking, house, bar, floor, distance, bus, bart, quiet, gate, easy, guest, available, district, public, golden, building, stay, 1, square, shop, just, unit, heart, youll, beautiful\n",
      "1: room, bedroom, bed, kitchen, ha, bathroom, living, private, home, san, apartment, large, 2, access, francisco, house, floor, guest, street, queen, block, space, park, area, neighborhood, located, view, 1, city, restaurant, great, tv, parking, dining, quiet, walk, spacious, available, bath, shared, away, flat, beautiful, comfortable, sf, mission, garden, place, stay, size, 3, downtown, open, deck, easy, just, unit, close, new\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 2.05 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF vectorizer\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-60:-1]\n",
    "print(\"top features (words) for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(features[i] for i in centroid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features (words) for each cluster:\n",
      "0: place, walk, locat, park, san, room, francisco, the, restaur, street, block, close, apart, kitchen, my, bedroom, citi, neighborhood, you, bed, we, i, great, access, sf, one, this, 2, view, travel, shop, privat, love, home, away, mission, downtown, 1, live, bart, bathroom, busi, space, minut, bar, hous, full, squar, good\n",
      "1: room, the, bedroom, park, kitchen, bed, live, locat, san, apart, walk, francisco, privat, bathroom, home, one, 2, access, block, street, neighborhood, i, floor, this, hous, restaur, full, we, larg, 1, guest, space, two, citi, area, queen, view, great, there, you, away, size, comfort, it, quiet, shop, mission, avail, share\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.83 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF-IDF vectorizer\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-50:-1]\n",
    "print(\"top features (words) for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(features[i] for i in centroid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
