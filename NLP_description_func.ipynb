{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline, randomly guessing that 20% of the data is popular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "my_list = [True for x in range(2)] +  [False for x in range(8)]\n",
    "accuracy_baseline = np.zeros(shape=(1,9))\n",
    "recall_baseline = np.zeros(shape=(1,9))\n",
    "precision_baseline = np.zeros(shape=(1,9))\n",
    "f1_baseline = np.zeros(shape=(1,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6760991542390486\n",
      "0.19911653217717348\n",
      "0.2076371303722557\n",
      "0.20267624173813048\n"
     ]
    }
   ],
   "source": [
    "df_sf_temp = df_sf_2017.copy()\n",
    "start_month = 1\n",
    "end_month = 4\n",
    "\n",
    "while end_month <13:\n",
    "    y_test = df_sf_temp[df_sf_temp['month'] == end_month]['popular']\n",
    "    y_pred = pd.Series(random.choice(my_list) for x in range(y_test.size))\n",
    "    accuracy_baseline[0][start_month-1] = accuracy_score(y_test, y_pred)\n",
    "    recall_baseline[0][start_month-1] = recall_score(y_test, y_pred)\n",
    "    precision_baseline[0][start_month-1] = precision_score(y_test, y_pred)\n",
    "    f1_baseline[0][start_month-1] = f1_score(y_test, y_pred)\n",
    "    start_month += 1\n",
    "    end_month += 1\n",
    "\n",
    "print(accuracy_baseline.mean())\n",
    "print(recall_baseline.mean()) \n",
    "print(precision_baseline.mean())\n",
    "print(f1_baseline.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datetime import datetime as dt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\",999)\n",
    "pd.set_option(\"display.max_rows\",999)\n",
    "pd.set_option(\"display.max_columns\",999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pickle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sf_2017 = pickle.load(open('../data_sf_2017.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017 = pd.read_json('df_sf_2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117262"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sf_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sf_2017.description.fillna(value='None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = df_sf_2017['description'].iloc[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer_train = CountVectorizer(analyzer=process_text).fit(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 95,\n",
       " 'room': 81,\n",
       " 'perfect': 68,\n",
       " 'locat': 52,\n",
       " 'enjoy': 27,\n",
       " 'peac': 66,\n",
       " 'environ': 29,\n",
       " 'want': 107,\n",
       " 'close': 19,\n",
       " 'shop': 89,\n",
       " 'restaur': 79,\n",
       " 'nightlif': 62,\n",
       " 'it': 46,\n",
       " 'one': 64,\n",
       " 'block': 13,\n",
       " 'j': 47,\n",
       " 'train': 98,\n",
       " '20m': 3,\n",
       " 'ride': 80,\n",
       " 'downtown': 24,\n",
       " '3': 4,\n",
       " 'mission': 57,\n",
       " 'the': 93,\n",
       " 'apart': 8,\n",
       " '2': 2,\n",
       " 'bedroom': 11,\n",
       " '1': 0,\n",
       " 'recent': 75,\n",
       " 'remodel': 76,\n",
       " 'bathroom': 10,\n",
       " 'there': 94,\n",
       " 'modern': 58,\n",
       " 'kitchen': 48,\n",
       " 'dishwash': 22,\n",
       " 'washer': 108,\n",
       " 'dryer': 25,\n",
       " 'build': 15,\n",
       " 'use': 103,\n",
       " 'best': 12,\n",
       " 'hill': 43,\n",
       " 'climb': 18,\n",
       " 'get': 37,\n",
       " 'feel': 31,\n",
       " 'light': 49,\n",
       " 'airi': 6,\n",
       " 'minimalist': 55,\n",
       " 'terrarium': 92,\n",
       " 'various': 105,\n",
       " 'plant': 70,\n",
       " 'scatter': 85,\n",
       " 'throughout': 96,\n",
       " 'you': 111,\n",
       " 'access': 5,\n",
       " 'entir': 28,\n",
       " 'place': 69,\n",
       " 'includ': 45,\n",
       " 'live': 51,\n",
       " 'dine': 21,\n",
       " 'garag': 35,\n",
       " 'share': 88,\n",
       " 'u': 102,\n",
       " 'two': 101,\n",
       " 'i': 44,\n",
       " 'happi': 41,\n",
       " 'help': 42,\n",
       " 'anyth': 7,\n",
       " 'need': 60,\n",
       " 'servic': 87,\n",
       " 'dog': 23,\n",
       " 'request': 77,\n",
       " 'guest': 40,\n",
       " 'plea': 71,\n",
       " 'respect': 78,\n",
       " 'quick': 74,\n",
       " 'go': 38,\n",
       " 'noe': 63,\n",
       " 'valley': 104,\n",
       " 'favorit': 30,\n",
       " 'neighborhood': 61,\n",
       " 'san': 83,\n",
       " 'francisco': 33,\n",
       " 'would': 110,\n",
       " 'love': 53,\n",
       " 'truli': 100,\n",
       " 'sen': 86,\n",
       " 'communiti': 20,\n",
       " 'peopl': 67,\n",
       " 'like': 50,\n",
       " 'say': 84,\n",
       " 'top': 97,\n",
       " 'floor': 32,\n",
       " 'breath': 14,\n",
       " 'take': 91,\n",
       " 'view': 106,\n",
       " 'easi': 26,\n",
       " 'transport': 99,\n",
       " 'great': 39,\n",
       " 'safe': 82,\n",
       " 'free': 34,\n",
       " 'park': 65,\n",
       " 'geograph': 36,\n",
       " 'center': 16,\n",
       " 'citi': 17,\n",
       " 'public': 73,\n",
       " 'avail': 9,\n",
       " '15': 1,\n",
       " 'minut': 56,\n",
       " 'privat': 72,\n",
       " 'within': 109,\n",
       " 'street': 90,\n",
       " 'muni': 59,\n",
       " 'market': 54}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer_train.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk\n",
      "hippo\n",
      "geese\n",
      "geese\n",
      "goose\n",
      "walked\n",
      "['walk', 'hippo', 'gees', 'gees', 'goos', 'walk']\n"
     ]
    }
   ],
   "source": [
    "text = 'walk you i san are to .be hippo geese. geese goose francisco walked'\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words.append('san')\n",
    "stop_words.append('francisco')\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "stemmed = []\n",
    "snowball = SnowballStemmer(\"english\")\n",
    "for item in tokens:\n",
    "    if item not in stop_words:\n",
    "        print(item)\n",
    "        stemmed.append(snowball.stem(item))\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def process_text(text):\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    stop_words.append('san')\n",
    "    stop_words.append('francisco')\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    stemmed = []\n",
    "    snowball = SnowballStemmer(\"english\")\n",
    "    for item in tokens:\n",
    "        if item not in stop_words:\n",
    "            stemmed.append(snowball.stem(item))\n",
    "        \n",
    "    lemmatized = []\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    for item in stemmed:\n",
    "        lemmatized.append(wordnet.lemmatize(item))\n",
    "    \n",
    "    return lemmatized\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data - running NLP on description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(start_month, end_month):\n",
    "    df_X_train = df_sf_2017[(df_sf_2017['month'] >= start_month) & (df_sf_2017['month'] < end_month)]['description']\n",
    "    y_train = df_sf_2017[(df_sf_2017['month'] >= start_month) & (df_sf_2017['month'] < end_month)]['popular']\n",
    "\n",
    "    df_X_test = df_sf_2017[df_sf_2017['month'] == end_month]['description']\n",
    "    y_test = df_sf_2017[df_sf_2017['month'] == end_month]['popular']\n",
    "    \n",
    "    return df_X_train, y_train, df_X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 18.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_tf_vec(df_X_train, df_X_test):\n",
    "    tf_vectorizer_train = CountVectorizer(analyzer=process_text).fit(df_X_train)\n",
    "    X_train = tf_vectorizer_train.transform(df_X_train)\n",
    "    tf_vectorizer_test = CountVectorizer(analyzer=process_text, vocabulary=tf_vectorizer_train.vocabulary_).fit(df_X_test)\n",
    "    X_test = tf_vectorizer_test.transform(df_X_test)\n",
    "    return X_train, X_test, tf_vectorizer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<26526x12784 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1848230 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_nb(X_train, y_train, X_test, y_test):\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    pickle.dump(nb, open('nb_model'+ str(model_num) + '.p', 'wb'))\n",
    "    preds = nb.predict(X_test)\n",
    "    scores_tf_nb[0][model_num] = accuracy_score(y_test, preds)\n",
    "    scores_tf_nb[1][model_num] = recall_score(y_test, nb.predict(X_test))\n",
    "    scores_tf_nb[2][model_num] = precision_score(y_test, nb.predict(X_test))\n",
    "    scores_tf_nb[3][model_num] = f1_score(y_test, nb.predict(X_test))\n",
    "    return scores_tf_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the CountVectorizer, run with RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_rf(X_train, y_train, X_test, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators = 10, n_jobs = -1, random_state=0, class_weight = {0:.95, 1:.05})\n",
    "    rf.fit(X_train, y_train)\n",
    "    predicted = rf.predict(X_test)\n",
    "    pickle.dump(rf, open('rf_nlp_countvec_50' + str(model_num) + '.p', 'wb'))\n",
    "    scores_tf_rf[0][model_num] = accuracy_score(y_test, predicted)\n",
    "    scores_tf_rf[1][model_num] = recall_score(y_test, predicted)\n",
    "    scores_tf_rf[2][model_num] = precision_score(y_test, predicted)\n",
    "    scores_tf_rf[3][model_num] = f1_score(y_test, predicted)\n",
    "    return scores_tf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tf_nb = np.zeros(shape=(4,9))\n",
    "scores_tf_rf = np.zeros(shape=(4,9))\n",
    "\n",
    "model_num = 0\n",
    "start_month = 1\n",
    "end_month = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 0\n",
      "tf_nb\n",
      "[[0.8589493  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.66435768 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.60320183 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.63230447 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_rf\n",
      "[[0.95539717 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.80730479 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.9398827  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.86856369 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "CPU times: user 2min 40s, sys: 1.21 s, total: 2min 42s\n",
      "Wall time: 2min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while end_month <13:\n",
    "    df_X_train, y_train, df_X_test, y_test = split_data(start_month, end_month)\n",
    "    X_train, X_test, tf_vectorizer_train = run_tf_vec(df_X_train, df_X_test)\n",
    "    scores_tf_nb = predict_tf_nb(X_train, y_train, X_test, y_test)\n",
    "    scores_tf_rf = predict_tf_rf(X_train, y_train, X_test, y_test)\n",
    "    print(start_month, end_month, model_num)\n",
    "    model_num += 1\n",
    "    start_month += 1\n",
    "    end_month += 1\n",
    "    print('tf_nb')\n",
    "    print(scores_tf_nb)\n",
    "    print('tf_rf')\n",
    "    print(scores_tf_rf)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8455697882468312\n",
      "0.6745968354038901\n",
      "0.6159944961116426\n",
      "0.6435671085940798\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_nb = np.mean(scores_tf_nb[0])\n",
    "recall_2017_tf_nb = np.mean(scores_tf_nb[1])\n",
    "precision_2017_tf_nb = np.mean(scores_tf_nb[2])\n",
    "f1_score_2017_tf_nb = np.mean(scores_tf_nb[3])\n",
    "print(accuracy_2017_tf_nb)\n",
    "print(recall_2017_tf_nb)\n",
    "print(precision_2017_tf_nb)\n",
    "print(f1_score_2017_tf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946939377339736\n",
      "0.7846092062627434\n",
      "0.9502124982965571\n",
      "0.8590649087408558\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_rf = np.mean(scores_tf_rf[0])\n",
    "recall_2017_tf_rf = np.mean(scores_tf_rf[1])\n",
    "precision_2017_tf_rf = np.mean(scores_tf_rf[2])\n",
    "f1_score_2017_tf_rf = np.mean(scores_tf_rf[3])\n",
    "print(accuracy_2017_tf_rf)\n",
    "print(recall_2017_tf_rf)\n",
    "print(precision_2017_tf_rf)\n",
    "print(f1_score_2017_tf_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try running with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def run_tf_idf_vec(df_X_train, df_X_test):\n",
    "    tf_idf_vectorizer_train = TfidfVectorizer(analyzer=process_text)\n",
    "    X_train2 = tf_idf_vectorizer_train.fit_transform(df_X_train).toarray()\n",
    "    tf_idf_vectorizer_test = TfidfVectorizer(analyzer=process_text, vocabulary=tf_idf_vectorizer_train.vocabulary_)\n",
    "    X_test2 = tf_idf_vectorizer_test.fit_transform(df_X_test).toarray()\n",
    "    return X_train2, X_test2, tf_idf_vectorizer_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf_nb(X_train2, y_train, X_test2, y_test):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(X_train2, y_train)\n",
    "    # pickle.dump(nb, open('nb_model_guassian.p', 'wb'))\n",
    "    preds = nb.predict(X_test2)\n",
    "    scores_tf_idf_nb[0][model_num] = accuracy_score(y_test, preds)\n",
    "    scores_tf_idf_nb[1][model_num] = recall_score(y_test, preds)\n",
    "    scores_tf_idf_nb[2][model_num] = precision_score(y_test, preds)\n",
    "    scores_tf_idf_nb[3][model_num] = f1_score(y_test, preds)\n",
    "    return scores_tf_idf_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the TF-IDF, run with RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tf_idf_rf(X_train2, y_train, X_test2, y_test):\n",
    "    rf = RandomForestClassifier(n_estimators = 10, n_jobs=-1, random_state=0, class_weight = {0:.95, 1:.05})\n",
    "    rf.fit(X_train2, y_train)\n",
    "    predicted = rf.predict(X_test2)\n",
    "#     pickle.dump(rf, open('rf_nlp_50.p', 'wb'))\n",
    "    scores_tf_idf_rf[0][model_num] = accuracy_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[1][model_num] = recall_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[2][model_num] = precision_score(y_test, predicted)\n",
    "    scores_tf_idf_rf[3][model_num] = f1_score(y_test, predicted)\n",
    "    return scores_tf_idf_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_tf_idf_nb = np.zeros(shape=(4,9))\n",
    "scores_tf_idf_rf = np.zeros(shape=(4,9))\n",
    "\n",
    "model_num = 0\n",
    "start_month = 1\n",
    "end_month = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 0\n",
      "tf_idf_nb\n",
      "[[0.61328888 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.93702771 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.31313131 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.46940063 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "tf_idf_rf\n",
      "[[0.94643062 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.76448363 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.9295559  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]\n",
      " [0.83897719 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n",
      "CPU times: user 6min 52s, sys: 3.94 s, total: 6min 56s\n",
      "Wall time: 2min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "while end_month <13:\n",
    "    df_X_train, y_train, df_X_test, y_test = split_data(start_month, end_month) \n",
    "    X_train2, X_test2, tf_idf_vectorizer_train = run_tf_idf_vec(df_X_train, df_X_test)\n",
    "    scores_tf_idf_nb = predict_tf_idf_nb(X_train2, y_train, X_test2, y_test)\n",
    "    scores_tf_idf_rf = predict_tf_idf_rf(X_train2, y_train, X_test2, y_test)\n",
    "    print(start_month, end_month, model_num)\n",
    "    model_num += 1\n",
    "    start_month += 1\n",
    "    end_month += 1\n",
    "    print('tf_idf_nb')\n",
    "    print(scores_tf_idf_nb)\n",
    "    print('tf_idf_rf')\n",
    "    print(scores_tf_idf_rf)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6223984688066541\n",
      "0.9360869500755025\n",
      "0.3467272958014258\n",
      "0.5052056786500755\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[0])\n",
    "recall_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[1])\n",
    "precision_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[2])\n",
    "f1_score_2017_tf_idf_nb = np.mean(scores_tf_idf_nb[3])\n",
    "print(accuracy_2017_tf_idf_nb)\n",
    "print(recall_2017_tf_idf_nb)\n",
    "print(precision_2017_tf_idf_nb)\n",
    "print(f1_score_2017_tf_idf_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9383264864764923\n",
      "0.7493591861607887\n",
      "0.9398408080684738\n",
      "0.8334356589996533\n"
     ]
    }
   ],
   "source": [
    "accuracy_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[0])\n",
    "recall_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[1])\n",
    "precision_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[2])\n",
    "f1_score_2017_tf_idf_rf = np.mean(scores_tf_idf_rf[3])\n",
    "print(accuracy_2017_tf_idf_rf)\n",
    "print(recall_2017_tf_idf_rf)\n",
    "print(precision_2017_tf_idf_rf)\n",
    "print(f1_score_2017_tf_idf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do I need k means? Or just top twenty words for popular and top twenty words for not popular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "features =  tf_idf_vectorizer_train.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1300',\n",
       " '1300sqft',\n",
       " '1304',\n",
       " '1315',\n",
       " '132',\n",
       " '135',\n",
       " '1350',\n",
       " '135cm',\n",
       " '136',\n",
       " '137',\n",
       " '1370123',\n",
       " '13724120',\n",
       " '139',\n",
       " '1394',\n",
       " '1396',\n",
       " '13ft',\n",
       " '13in',\n",
       " '13lb',\n",
       " '13mn',\n",
       " '13th',\n",
       " '13x12',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '1400ft²',\n",
       " '1400sf',\n",
       " '1408',\n",
       " '140sq',\n",
       " '140sqm',\n",
       " '14154147',\n",
       " '143',\n",
       " '144',\n",
       " '145',\n",
       " '1450',\n",
       " '14611926',\n",
       " '1474',\n",
       " '149',\n",
       " '14ft',\n",
       " '14l',\n",
       " '14min',\n",
       " '14r',\n",
       " '14sqm',\n",
       " '14th',\n",
       " '14x',\n",
       " '14x12',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '1500ft',\n",
       " '1500sf']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[150:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 736 ms, sys: 264 ms, total: 1e+03 ms\n",
      "Wall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kmeans = KMeans(n_clusters=2, n_jobs=-1)\n",
    "kmeans.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features (words) for each cluster:\n",
      "0: place, walk, locat, park, san, room, francisco, the, restaur, street, block, close, apart, kitchen, my, bedroom, citi, neighborhood, you, bed, we, i, great, access, sf, one, this, 2, view, travel, shop, privat, love, home, away, mission, downtown, 1, live, bart, bathroom, busi, space, minut, bar, hous, full, squar, good, share, coupl, it, check, build, stay, avail, distanc, area, heart\n",
      "1: room, the, bedroom, park, kitchen, bed, live, locat, san, apart, walk, francisco, privat, bathroom, home, one, 2, access, block, street, neighborhood, i, floor, this, hous, restaur, full, we, larg, 1, guest, space, two, citi, area, queen, view, great, there, you, away, size, comfort, it, quiet, shop, mission, avail, share, also, beauti, sf, includ, stay, tv, dine, minut, unit, bath\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 2.13 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF-IDF vectorizer\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-60:-1]\n",
    "print(\"top features (words) for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(features[i] for i in centroid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top features (words) for each cluster:\n",
      "0: place, walk, locat, park, san, room, francisco, the, restaur, street, block, close, apart, kitchen, my, bedroom, citi, neighborhood, you, bed, we, i, great, access, sf, one, this, 2, view, travel, shop, privat, love, home, away, mission, downtown, 1, live, bart, bathroom, busi, space, minut, bar, hous, full, squar, good\n",
      "1: room, the, bedroom, park, kitchen, bed, live, locat, san, apart, walk, francisco, privat, bathroom, home, one, 2, access, block, street, neighborhood, i, floor, this, hous, restaur, full, we, larg, 1, guest, space, two, citi, area, queen, view, great, there, you, away, size, comfort, it, quiet, shop, mission, avail, share\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 1.83 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TF vectorizer\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-50:-1]\n",
    "print(\"top features (words) for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(features[i] for i in centroid)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
